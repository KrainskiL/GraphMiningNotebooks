{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required extra package:\n",
    "\n",
    "For hypergraphs:\n",
    "* pip install hypernetx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import hypernetx as hnx\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data directory\n",
    "datadir='../Datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of extra functions for HNX hypergraphs\n",
    "\n",
    "### Build hypergraph and pre-compute key quantities\n",
    "\n",
    "We build the hypergraph HG using:\n",
    "\n",
    "```python\n",
    "HG = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "```\n",
    "\n",
    "where 'Edges' is a list of sets; edges are then indexed as 0-based integers,\n",
    "so to preserve unique ids, we represent nodes as strings.\n",
    "For example Edges[0] = {'0','2'}\n",
    "\n",
    "Once the HNX hypergraph is built, the following function is called to \n",
    "compute node strengths, d-degrees and binomial coefficients:\n",
    "\n",
    "```python\n",
    "HNX_precompute(HG)\n",
    "```\n",
    "\n",
    "### Partitions\n",
    "\n",
    "We use two representations for partitions: list of sets (the parts) or dictionary.\n",
    "Those functions are used to map from one to the other:\n",
    "\n",
    "```python\n",
    "dict2part(D)\n",
    "part2dict(A)\n",
    "```\n",
    "\n",
    "### H-modularity\n",
    "\n",
    "The function to compute H-modularity for HG w.r.t. partition A (list of sets covering the vertices):\n",
    "\n",
    "```python\n",
    "HNX_modularity(HG, A, wcd=linear)\n",
    "```\n",
    "\n",
    "where 'wcd' is the weight function (default = 'linear'). Other choices are 'strict'\n",
    "and 'majority', or any user-supplied function with the following format:\n",
    "\n",
    "```python\n",
    "def linear(d,c):\n",
    "    return c/d if c>d/2 else 0\n",
    "```\n",
    "\n",
    "where d is the edge size, and d>=c>d/2 the number of nodes in the majority class.\n",
    "\n",
    "### Two-section graph\n",
    "\n",
    "Build the random-walk based 2-section graph given some hypergraph HG:\n",
    "\n",
    "```python\n",
    "G = HNX_2section(HG)\n",
    "```\n",
    "\n",
    "where G is an igraph Graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for HNX nypergraphs as described above:\n",
    "\n",
    "def factorial(n): \n",
    "    if n < 2: return 1\n",
    "    return reduce(lambda x, y: x*y, range(2, int(n)+1))\n",
    "\n",
    "## Precompute soe values on HNX hypergraph for computing qH faster\n",
    "def HNX_precompute(HG):\n",
    "    ## 1. compute node strenghts (weighted degrees)\n",
    "    for v in HG.nodes:\n",
    "        HG.nodes[v].strength = 0\n",
    "    for e in HG.edges:\n",
    "        try:\n",
    "            w = HG.edges[e].weight\n",
    "        except:\n",
    "            w = 1\n",
    "            ## add unit weight if none to simplify other functions\n",
    "            HG.edges[e].weight = 1 \n",
    "        for v in list(HG.edges[e]):\n",
    "            HG.nodes[v].strength += w\n",
    "    ## 2. compute d-weights        \n",
    "    ctr = Counter([len(HG.edges[e]) for e in HG.edges])\n",
    "    for k in ctr.keys():\n",
    "        ctr[k]=0\n",
    "    for e in HG.edges:\n",
    "        ctr[len(HG.edges[e])] += HG.edges[e].weight\n",
    "    HG.d_weights = ctr\n",
    "    HG.total_weight = sum(ctr.values())\n",
    "    ## 3. compute binomial coeffcients (modularity speed-up)\n",
    "    bin_coef = {}\n",
    "    for n in HG.d_weights.keys():\n",
    "        for k in np.arange(n//2+1,n+1):\n",
    "            bin_coef[(n,k)] = factorial(n)/(factorial(k)*factorial(n-k))\n",
    "    HG.bin_coef = bin_coef\n",
    "\n",
    "#########################################\n",
    "\n",
    "## some weight function 'wdc' for d-edges with c-majority\n",
    "\n",
    "## default: linear w.r.t. c\n",
    "def linear(d,c):\n",
    "    return c/d if c>d/2 else 0\n",
    "\n",
    "## majority\n",
    "def majority(d,c):\n",
    "    return 1 if c>d/2 else 0\n",
    "\n",
    "## strict\n",
    "def strict(d,c):\n",
    "    return 1 if c==d else 0\n",
    "\n",
    "#########################################\n",
    "\n",
    "## compute vol(A_i)/vol(V) for each part A_i in A (list of sets)\n",
    "def compute_partition_probas(HG, A):\n",
    "    p = []\n",
    "    for part in A:\n",
    "        vol = 0\n",
    "        for v in part:\n",
    "            vol += HG.nodes[v].strength\n",
    "        p.append(vol)\n",
    "    s = sum(p)\n",
    "    return [i/s for i in p]\n",
    "\n",
    "## degree tax \n",
    "def DegreeTax(HG, Pr, wdc):\n",
    "    DT = 0\n",
    "    for d in HG.d_weights.keys():\n",
    "        tax = 0\n",
    "        for c in np.arange(d//2+1,d+1):\n",
    "            for p in Pr:\n",
    "                tax += p**c * (1-p)**(d-c) * HG.bin_coef[(d,c)] * wdc(d,c)\n",
    "        tax *= HG.d_weights[d]\n",
    "        DT += tax\n",
    "    DT /= HG.total_weight\n",
    "    return DT\n",
    "\n",
    "## edge contribution, A is list of sets\n",
    "def EdgeContribution(HG, A, wdc):\n",
    "    EC = 0\n",
    "    for e in HG.edges:\n",
    "        d = HG.size(e)\n",
    "        for part in A:\n",
    "            if HG.size(e,part) > d/2:\n",
    "                EC += wdc(d,HG.size(e,part)) * HG.edges[e].weight\n",
    "    EC /= HG.total_weight\n",
    "    return EC\n",
    "\n",
    "## HG: HNX hypergraph\n",
    "## A: partition (list of sets)\n",
    "## wcd: weight function (ex: strict, majority, linear)\n",
    "def HNX_modularity(HG, A, wdc=linear):\n",
    "    Pr = compute_partition_probas(HG, A)\n",
    "    return EdgeContribution(HG, A, wdc) - DegreeTax(HG, Pr, wdc)\n",
    "\n",
    "#########################################\n",
    "\n",
    "## 2-section igraph from HG\n",
    "def HNX_2section(HG):\n",
    "    s = []\n",
    "    for e in HG.edges:\n",
    "        E = HG.edges[e]\n",
    "         ## random-walk 2-section (preserve nodes' weighted degrees)\n",
    "        try:\n",
    "            w = HG.edges[e].weight/(len(E)-1)\n",
    "        except:\n",
    "            w = 1/(len(E)-1)\n",
    "        s.extend([(k[0],k[1],w) for k in itertools.combinations(E,2)])\n",
    "    G = ig.Graph.TupleList(s,weights=True).simplify(combine_edges='sum')\n",
    "    return G\n",
    "\n",
    "#########################################\n",
    "\n",
    "## we use 2 representations for partitions (0-based part ids):\n",
    "## (1) dictionary or (2) list of sets\n",
    "\n",
    "def dict2part(D):\n",
    "    P = []\n",
    "    k = list(D.keys())\n",
    "    v = list(D.values())\n",
    "    for x in range(max(D.values())+1):\n",
    "        P.append(set([k[i] for i in range(len(k)) if v[i]==x]))\n",
    "    return P\n",
    "\n",
    "def part2dict(A):\n",
    "    x = []\n",
    "    for i in range(len(A)):\n",
    "        x.extend([(a,i) for a in A[i]])\n",
    "    return {k:v for k,v in x}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy hypergraph example with HNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "## using 'enumerate', edges will have integer IDs\n",
    "E = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "HD = HG.dual()\n",
    "hnx.draw(HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute node strength (add unit weight if none), d-degrees, binomial coefficients\n",
    "HNX_precompute(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the edges (unit weights added by default)\n",
    "HG.edges.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the nodes (here strength = degree since all weights are 1)\n",
    "HG.nodes.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d-weights\n",
    "HG.d_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute modularity qH for the following partitions:\n",
    "A1 = [{'A','B','C'},{'D','E','F'}]\n",
    "A2 = [{'B','C'},{'A','D','E','F'}]\n",
    "A3 = [{'A','B','C','D','E','F'}]\n",
    "A4 = [{'A'},{'B'},{'C'},{'D'},{'E'},{'F'}]\n",
    "\n",
    "print('linear:',HNX_modularity(HG,A1),HNX_modularity(HG,A2),HNX_modularity(HG,A3),HNX_modularity(HG,A4))\n",
    "print('strict:',HNX_modularity(HG,A1,strict),HNX_modularity(HG,A2,strict),HNX_modularity(HG,A3,strict),HNX_modularity(HG,A4,strict))\n",
    "print('majority:',HNX_modularity(HG,A1,majority),HNX_modularity(HG,A2,majority),HNX_modularity(HG,A3,majority),HNX_modularity(HG,A4,majority))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = HNX_2section(HG)\n",
    "G.vs['label'] = G.vs['name']\n",
    "ig.plot(G,bbox=(0,0,250,250))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section clustering with ECG\n",
    "G.vs['community'] = G.community_ecg().membership\n",
    "dict2part({v['name']:v['community'] for v in G.vs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "REF: https://github.com/jeffreylancaster/game-of-thrones\n",
    "\n",
    "We built an hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving those characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the GoT data\n",
    "Edges, Names, Weights = pickle.load(open( datadir+\"Got/GoT.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build weighted hypergraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "HG = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "## add edge weights\n",
    "for e in HG.edges:\n",
    "    HG.edges[e].weight = Weights[e]\n",
    "## add full names\n",
    "for v in HG.nodes:\n",
    "    HG.nodes[v].name = Names[v]\n",
    "## pre-compute required quantities for modularity and clustering\n",
    "HNX_precompute(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HG.number_of_nodes(),'nodes and',HG.number_of_edges(),'edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on GoT hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "plt.hist([HG.edges[e].size() for e in HG.edges], bins=25)\n",
    "plt.xlabel(\"Edge size\",fontsize=14);\n",
    "#plt.savefig('got_hist_1.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([HG.edges[e].weight for e in HG.edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters)\n",
    "plt.hist([HG.edges[e].weight for e in HG.edges], bins=25)\n",
    "plt.xlabel(\"Edge weight\",fontsize=14);\n",
    "#plt.savefig('got_hist_2.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## node degrees\n",
    "plt.hist(hnx.degree_dist(HG),bins=20)\n",
    "plt.xlabel(\"Node degree\",fontsize=14);\n",
    "#plt.savefig('got_hist_3.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node strength (total appearance)\n",
    "plt.hist([HG.nodes[n].strength for n in HG.nodes], bins=20)\n",
    "plt.xlabel(\"Node strength\",fontsize=14);\n",
    "#plt.savefig('got_hist_4.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build dataframe with node characteristics\n",
    "dg = [HG.degree(v) for v in HG.nodes()]\n",
    "st = [HG.nodes[v].strength for v in HG.nodes()]\n",
    "nm = [HG.nodes[v].name for v in HG.nodes()]\n",
    "D = pd.DataFrame(np.array([nm,dg,st]).transpose(),columns=['name','degree','strength'])\n",
    "D['degree'] = pd.to_numeric(D['degree'])\n",
    "D['strength'] = pd.to_numeric(D['strength'])\n",
    "D.sort_values(by='strength',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sort_values(by='degree',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(D['degree'],D['strength'],'.')\n",
    "plt.xlabel('degree',fontsize=14)\n",
    "plt.ylabel('strength',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "G = HNX_2section(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check node ordering \n",
    "\n",
    "## ordering of nodes in HG\n",
    "ord_HG = list(HG.nodes.elements.keys())\n",
    "\n",
    "## ordering of nodes in G\n",
    "ord_G = [v['name'] for v in G.vs]\n",
    "\n",
    "ord_HG == ord_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = G.betweenness(directed=False,weights='weight')\n",
    "n = G.vcount()\n",
    "D['betweenness'] = [2*x/((n-1)*(n-2)) for x in b]\n",
    "D['pagerank'] = G.pagerank(directed=False,weights='weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sort_values(by='strength',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## To produce LaTeX from a DataFrame\n",
    "df = D.sort_values(by='strength',ascending=False).head(10).round(decimals=4)\n",
    "print(df.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sort_values(by='betweenness',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(D['pagerank'],D['betweenness'],'.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the 2-section graph\n",
    "\n",
    "print('nodes:',G.vcount(),'edges:',G.ecount())\n",
    "G.vs['size'] = 10\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [HG.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 5\n",
    "ly = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout = ly, bbox=(0,0,600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a small clique: Braavosi theater troup\n",
    "print([HG.nodes[str(x)].name for x in np.arange(166,173)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modularity (qH) on a random partition with K parts\n",
    "## This should be close to 0 and can be negative.\n",
    "h = []\n",
    "for K in np.arange(2,21):\n",
    "    for rep in range(10):\n",
    "        V = list(HG.nodes)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## compute qH\n",
    "        h.append(HNX_modularity(HG, RandPart))\n",
    "print(min(h),max(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the 2-section graph (with Louvain) and compute qH\n",
    "## We now see qH >> 0\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "D['cluster'] = G.vs['louvain']\n",
    "ML = dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "## Compute qH\n",
    "print(HNX_modularity(HG, ML))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot w.r.t. the resulting clusters\n",
    "cl = G.vs['louvain']\n",
    "pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "#pal = ig.ClusterColoringPalette(max(cl)+1)\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "G.vs['label_size'] = 0\n",
    "ig.plot(G, layout = ly, bbox=(0,0,500,400))\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly, bbox=(0,0,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex: high strength nodes in cluster with Daenerys Targaryen\n",
    "dt = int(D[D['name']=='Daenerys Targaryen']['cluster'])\n",
    "D[D['cluster']==dt].sort_values(by='strength',ascending=False).head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## To produce LaTeX from a DataFrame\n",
    "df = D[D['cluster']==dt].sort_values(by='strength',ascending=False).head().round(decimals=4)\n",
    "print(df.to_latex(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
