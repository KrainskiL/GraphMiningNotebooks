{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Hypergraphs\n",
    "\n",
    "In this notebook, we introduce hypergraphs, a generalization of graphs where we allow for arbitrary sized edges (in practice, we usually consider only edges of size 2 or more). \n",
    "\n",
    "We illustrate a few concepts using hypergraphs including modularity, community detection, simpliciality and transformation into 2-section graphs.\n",
    "\n",
    "**This notebook requires version 2.3 or newer of the HyperNetX package** (https://github.com/pnnl/HyperNetX).\n",
    "\n",
    "We also do some visualization with **XGI** (https://xgi.readthedocs.io/en/stable/index.html), which can by pip installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip uninstall hypernetx\n",
    "pip install git+https://github.com/pnnl/HyperNetX.git@master\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hypernetx as hnx\n",
    "import hypernetx.algorithms.hypergraph_modularity as hmod \n",
    "import xgi ## pip install xgi is required\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from scipy.special import comb\n",
    "import warnings\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import fastnode2vec as n2v\n",
    "import umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## Functions to compute various simpliciality measures are found in the included ```simpliciality.py``` file:\n",
    "import simpliciality as spl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set this to the data directory\n",
    "datadir='../Datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to compute degree-size correlation\n",
    "def h_deg_size_corr(H):\n",
    "    deg = {v:H.degree(v) for v in H.nodes}\n",
    "    X = []\n",
    "    Y = []\n",
    "    for e in H.edges:\n",
    "        for v in H.edges[e]:\n",
    "            X.append(deg[v])\n",
    "            Y.append(len(H.edges[e]))\n",
    "    return(np.corrcoef(X,Y)[1,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperNetX basics with a toy hypergraph\n",
    "\n",
    "We illustrate a few concepts with a small toy hypergraph. \n",
    "\n",
    "First, we build the HNX hypergraph from a list of sets (the edges), and we draw the hypergraph as well as its dual (where the role of nodes and edges are swapped).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "## using 'enumerate', edges will have integer IDs\n",
    "E = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'},{'B'},{'G','B'}]\n",
    "kwargs = {'layout_kwargs': {'seed': 4321}, 'with_node_counts': False}\n",
    "H = hnx.Hypergraph(dict(enumerate(E)))\n",
    "for e in H.edges:\n",
    "    H.edges[e].weight = 1.0\n",
    "edges_kwargs={'edgecolors':'black'}\n",
    "with warnings.catch_warnings(): ## matplotlib warning\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plt.figure(figsize=(5,5))\n",
    "hnx.draw(H, **kwargs, edges_kwargs=edges_kwargs, edge_label_alpha=1)\n",
    "#plt.savefig('h_toy_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "H_dual = H.dual()\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_node_counts': False, 'with_edge_labels':True}\n",
    "sizes = np.array([H_dual.size(e) for e in H_dual.edges])\n",
    "norm = plt.Normalize(sizes.min(),sizes.max())\n",
    "edges_kwargs={'edgecolors':'black'}\n",
    "plt.figure(figsize=(5,5))\n",
    "hnx.draw(H_dual, **kwargs, edges_kwargs=edges_kwargs, edge_label_alpha=1)\n",
    "#plt.savefig('h_toy_b.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the nodes and edges\n",
    "print('shape:', H.shape)\n",
    "print('nodes:', [x for x in H.nodes()])\n",
    "print('edges:', [x for x in H.edges()])\n",
    "print('node degrees:', [(v,H.degree(v)) for v in H.nodes()])\n",
    "print('edge sizes:',[H.size(e) for e in H.edges()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## incidence dictionary\n",
    "H.incidence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## incidence matrix\n",
    "df = pd.DataFrame(H.incidence_matrix().toarray(), \n",
    "                  index=[v for v in H.nodes()],\n",
    "                  columns=['edge_'+str(i) for i in np.arange(0,8)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = hmod.two_section(H)\n",
    "ig.plot(G, bbox=(300,300),vertex_label=G.vs['name'], vertex_label_size=9, vertex_color='lightblue')\n",
    "#ig.plot(G, target=\"h_toy_c.png\", bbox=(300,300),vertex_label=G.vs['name'], vertex_label_size=9, vertex_color='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s-walks are distance-based measures\n",
    "\n",
    "We illustrate a few concepts with the toy hypergraph defined earlier.\n",
    "\n",
    "Let H=(V,E) a hypergraph, and consider its incidence matrix $B$ defined in section 7.2. \n",
    "Consider also the dual hypergraph $H^*$, where the roles of nodes are hyperedges are swapped, \n",
    "namely the edges in $H$ are the nodes in $H^*$, \n",
    "and there is as edge two vertices in $H^*$ if the corresponding hyperedges in $H$ have a non-empty intersection.\n",
    "In the cells below, we show a toy hypergraph (with nodes labelled with letters A to G, \n",
    "and edges labelled with digits 0 to 7. We also show the corresponding dual hypergraph, and the 2-section graph.\n",
    "\n",
    "### s-walks and distances\n",
    "\n",
    "We define the concept of $s$-walks on a hypergraph as follows. A $s$-**walk** of length $k$ on $H$ is a sequences of edges $e_{i_0}, e_{i_1}, ..., e_{i_k}$ in $E$ such that \n",
    "all $|e_{i_{j-1}} \\cap e_{i_j}| \\ge s$ for $1 \\le j \\le k$ and all $i_{j-1} \\ne i_j$.\n",
    "\n",
    "The $s$-**distance** $d_s(e_i,e_j)$ between edges $e_i$ and $e_j$ is the length of the smallest $s$-walk between those, if it exists (else the distance is usually considered as infinity, and its inverse is set to zero).\n",
    "\n",
    "A subset $E_s \\subset E$ is an $s$-**connected component** if it is a maximal subset with as $s$-walk between all $e_i, e_j \\in E_s$.\n",
    "The $s$-**diameter** for $E_s$ is the maximal shortest path length between all $e_i, e_j \\in E_s$.\n",
    "\n",
    "Other concepts can also be defined using $s$-walks. For example for distinct $e_i, e_j, e_k \\in E$, if there is a $s$-walk $e_i, e_j, e_k$, we say that they form an $s$-**wedge**, and if there is an $s$ walk $e_i, e_j, e_k, e_i$, we can say those form an $s$-**triangle** and from those, we can define the $s$-clustering coefficients as we saw in Section 1.11.\n",
    "\n",
    "For **nodes**, all definitions above follow by considering the **dual** hypergraph. For example, a $s$-walk is a sequence of adjacent nodes such that each consecutive node pair in the walk share at least $s$ hyperedges; all other concepts defined above follow directly.\n",
    "\n",
    "#### toy example\n",
    "\n",
    "In the toy example above, with $s=2$, the sequence of edges 1-2-0 is a $s$-path since edges 1 and 2 share nodes A and C, and edges 2 and 0 share nodes A and B.\n",
    "\n",
    "In the dual toy hypergraph, again with $s=2$, the sequence of nodes (edges in the dual) D-F-E is a $s$-path since nodes D and F are both adjacent to edges 3 and 4 in H, and nodes F and E are both adjacent to edges 3 and 5 in H. \n",
    "Another $s$-path (with $s=2$) is C-A-B.\n",
    "\n",
    "With $s=1$, this corresponds to a walk on the (unweighted 2-section) graph, while for $s \\ge 2$, this only applies to hypergraphs.\n",
    "\n",
    "Below, we compute the distances between every pair of nodes (thus, using the $s$-walks on the dual). An infinite distance between a pair of nodes means that there is no $s$-path joining those.\n",
    "\n",
    "We see the correspondence between the $s=1$ and graph cases; moreover in those cases, we have a single connected component since every pairs of nodes is connected by a path.\n",
    "\n",
    "With $s=2$, we see several disconnected node pairs, so in this case, we have several $s$-connected components. From inspection of the table above, we see that nodes {A,B,C} are connected,\n",
    "nodes {D,E,F} are connected; node G is then an isolated node. We verify this claim below (we also do the same with the edges, i.e. using the $s$-walk on $H$ with $s=2$)                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distances with s=1 and s=2 and on the 2-section graph\n",
    "warnings.filterwarnings('ignore')\n",
    "Nodes = ['A','B','C','D','E','F','G']\n",
    "L = []\n",
    "for i in range(len(Nodes)-1):\n",
    "    for j in np.arange(i+1,len(Nodes)):\n",
    "        L.append([Nodes[i],Nodes[j],G.distances(Nodes[i],Nodes[j])[0][0],H.distance(Nodes[i],Nodes[j]),H.distance(Nodes[i],Nodes[j],s=2)])\n",
    "df = pd.DataFrame(L, columns=['node1','node2','2-section','s=1','s=2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## s=2 components\n",
    "Edges = [cc for cc in H.s_connected_components(s=2)]\n",
    "Nodes = [cc for cc in H.s_connected_components(s=2, edges=False)]\n",
    "print('s=2, components for the nodes:',Nodes)\n",
    "print('s=2, components for the edges:',Edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## line graph\n",
    "\n",
    "Below we illustrate the line graph for the toy hypergraph with $s=2$.\n",
    "\n",
    "Recall that in a line graph, the nodes are the edges in the original hypergraph, \n",
    "and an edge is draw between those if they share at least $s$ nodes in the original hypergraph.\n",
    "\n",
    "Thus we see the connectd components as listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG = ig.Graph.from_networkx(H.get_linegraph(s=2))\n",
    "ig.plot(LG, bbox=(200,200),vertex_label=LG.vs['_nx_name'], vertex_label_size=9, vertex_color='lightgrey')\n",
    "ig.plot(LG, target=\"h_toy_d.png\", bbox=(200,200),vertex_label=LG.vs['_nx_name'], vertex_label_size=9, vertex_color='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example - centralities\n",
    "\n",
    "For $H=(V,E)$, we define the (un-normalized) **$s$-harmonic centrality** for edge $e_i \\in E$ as:\n",
    "$\\sum_{e_j \\in E_s; e_i \\ne e_j} \\frac{1}{d_s(e_i,e_j)}$.\n",
    "It can be normalized by dividing by ${|E|-1}$.\n",
    "Recall that for $s$-disconnected edges $e_i, e_j$, we set $\\frac{1}{d_s(e_i,e_j)} = 0$.\n",
    "\n",
    "For nodes, the definition is identical using the dual hypergraph.\n",
    "For our toy example, with $s=2$, nodes {A,B,C} form a connected connected component as we saw earlier, same\n",
    "for nodes {D,E,F}, while node G is an isolated node.\n",
    "\n",
    "Looking at the table of distances we computed earlier, we see that $d_2(A,B)=d_2(A,C)=1$ and $d_2(B,C)$=2,\n",
    "so before normalization, the harmonic centrality for A is 2, and for B and C it is 1.5.\n",
    "Results are comparable for the other connected component, with values of 1.5 for nodes D and E, and 2 for node F.\n",
    "Node G is isolated and thus has zero harmonic centrality.\n",
    "\n",
    "We can also define $s$-**betweenness centrality** as we did for graphs, namely for edge $e_i \\in E$:\n",
    "\n",
    "$\\sum_{e_j \\in E-\\{e_i\\}} \\sum_{e_k \\in E-\\{e_i, e_j\\}} \\frac{\\ell(e_j,e_k,e_i)}{\\ell(e_j,e_k)}$\n",
    "\n",
    "where: $\\ell(e_j,e_k)$ is the number of shortest $s$-paths between $e_j$ and $e_k$, \n",
    "and $\\ell(e_j,e_k,e_i)$ is the number of shortest $s$-paths between $e_j$ and $e_k$ that include $e_i$.\n",
    "Again the definition is the same for nodes using the dual hypergraph.\n",
    "\n",
    "For our toy example, with $s=2$, the only nodes that are on shortest $s$-paths between other nodes are nodes A (between B and C)\n",
    "and node F (between D and E), thus the results we see below.\n",
    "\n",
    "Other distance-based centrality measures can be defined for hypergraphs in the same way, using $s$-distances,\n",
    "including the measures we covered in Section 3.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## centralities with some 's' value\n",
    "s = 2\n",
    "\n",
    "hc = hnx.algorithms.s_harmonic_centrality(H, edges=False, s=s, normalized=False)\n",
    "bc = hnx.algorithms.s_betweenness_centrality(H, edges=False, s=s, normalized=False)\n",
    "cc = hnx.algorithms.s_closeness_centrality(H, edges=False, s=s)\n",
    "\n",
    "D = pd.DataFrame([[v,hc[v],cc[v],bc[v]] for v in H.nodes], columns=['node','harmonic','closeness','betweenness'])\n",
    "#print(D.sort_values('harmonic', ascending=False).to_latex())\n",
    "D.sort_values('harmonic', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hypergraph modularity qH\n",
    "\n",
    "We compute qH on the toy graph for 4 different partitions, and using different variations for the edge contribution.\n",
    "\n",
    "For edges of size $d$ where $c$ is the number of nodes from the part with the most representatives, we consider  variations as follows for edge contribution:\n",
    "\n",
    "* **strict**: edges are considered only if all nodes are from the same part, with unit weight, i.e. $w$ = 1 iff $c == d$ (0 else).\n",
    "* **cubic**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the cube of the number of nodes in the majority, i.e. $w = (c/d)^3$ iff $c>d/2$ (0 else).\n",
    "* **quadratic**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the square of the number of nodes in the majority, i.e. $w = (c/d)^2$ iff $c>d/2$ (0 else).\n",
    "* **linear**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the number of nodes in the majority, i.e. $w = c/d$ iff $c>d/2$ (0 else).\n",
    "* **majority**: edges are counted only if more that half the nodes are from the same part, with unit weights, i.e. $w$ = 1 iff $c>d/2$ (0 else).\n",
    "\n",
    "Some of the above are supplied with the `hmod` module, the **qH2** and **qH3** functions are examples of user-supplied choice.\n",
    "\n",
    "The order above goes from only counting \"pure\" edges as community edges, gradually giving more weight to edges with $c>d/2$, all the way to giving the the same weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## these will be included in the next version of hmod\n",
    "## square modularity \n",
    "def qH2(d,c):\n",
    "    return (c/d)**2 if c > d/2 else 0\n",
    "## cubic modularity\n",
    "def qH3(d,c):\n",
    "    return (c/d)**3 if c > d/2 else 0\n",
    "\n",
    "## compute hypergraph modularity (qH) for the following partitions:\n",
    "A1 = [{'A','B','C','G'},{'D','E','F'}]            ## good clustering, qH should be positive\n",
    "A2 = [{'B','C'},{'A','D','E','F','G'}]            ## not so good\n",
    "A3 = [{'A','B','C','D','E','F','G'}]              ## this should yield qH == 0\n",
    "A4 = [{'A'},{'B'},{'C'},{'D'},{'E'},{'F'},{'G'}]  ## qH should be negative here\n",
    "\n",
    "## we compute with different choices of functions for the edge contribution\n",
    "\n",
    "print('strict edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.strict)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.strict)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.strict)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.strict)))\n",
    "print('\\ncubic edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,qH3)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,qH3)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,qH3)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,qH3)))\n",
    "print('\\nquadratic edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,qH2)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,qH2)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,qH2)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,qH2)))\n",
    "print('\\nlinear edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.linear)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.linear)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.linear)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.linear)))\n",
    "print('\\nmajority edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.majority)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.majority)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.majority)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.majority)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted 2-section graph\n",
    "\n",
    "We build the 2-section graph for the above toy hypergraph, and run some graph custering on this graph.\n",
    "\n",
    "We see different edge weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = hmod.two_section(H)\n",
    "G.vs['label'] = G.vs['name']\n",
    "ig.plot(G, bbox=(0,0,250,250), edge_width = 2*np.array(G.es['weight']), \n",
    "        vertex_color='gainsboro', vertex_label_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section clustering with Leiden\n",
    "G.vs['community'] = G.community_leiden(objective_function='modularity', weights='weight').membership\n",
    "print('clusters:',hmod.dict2part({v['name']:v['community'] for v in G.vs}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kumar clustering\n",
    "cl = hmod.kumar(H)\n",
    "cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h-ABCD Examples\n",
    "\n",
    "The first small h-ABCD hypergraph we use next was generated as follows:\n",
    "\n",
    "`julia --project abcdh.jl -n 100 -d 2.5,3,10 -c 1.5,30,40 -x .2 -q 0,.3,.4,.3 -w :strict -s 123 -o toy_100`\n",
    "\n",
    "It has 100 nodes and 3 well-defined communities. We will use this example mainly for visualization.\n",
    "\n",
    "The second one, which is much more noisy, was generated as follows:\n",
    "\n",
    "`julia --project abcdh.jl -n 300 -d 2.5,5,30 -c 1.5,80,120 -x .6 -q 0,0,.1,.9 -w :strict -s 123 -o toy_300`\n",
    "\n",
    "We will use this example to show that optimizing the appropriate hypergraph modularity function can lead to better clustering in some cases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100-node h-ABCD - visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the edges and build the h-ABCD hypergraph H\n",
    "fp = open(datadir+'ABCD/toy_100_he.txt', 'r')\n",
    "\n",
    "Lines = fp.readlines()\n",
    "Edges = []\n",
    "for line in Lines:\n",
    "    #Edges.append(set([int(x)-1 for x in line.strip().split(',')]))\n",
    "    Edges.append(set([x for x in line.strip().split(',')]))\n",
    "H = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "print('distribution of edge sizes:',Counter([len(x) for x in Edges]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the ground-truth communities and assign node colors accordingly\n",
    "H_comm = {str(k+1):v for k,v in enumerate(pd.read_csv(datadir+'ABCD/toy_100_assign.txt', header=None)[0].tolist())}\n",
    "cls = ['white','darkgrey','black']\n",
    "node_colors = dict(zip(H.nodes, [cls[H_comm[i]-1] for i in H.nodes]))\n",
    "\n",
    "## build the 2-section graph and plot (with ground-truth community colors)\n",
    "g = hmod.two_section(H)\n",
    "for v in g.vs:\n",
    "    v['color'] = node_colors[v['name']]\n",
    "    v['gt'] = H_comm[v['name']]\n",
    "    \n",
    "random.seed(12345)\n",
    "ly = g.layout_fruchterman_reingold()\n",
    "g.vs['ly'] = [x for x in ly]\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ig.plot(g, target=ax, vertex_size=9, layout=ly, edge_color='darkgrey', edge_width=1);\n",
    "#fig.savefig('habcd_1.pdf');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## rubber band plot\n",
    "H_ly = dict(zip(g.vs['name'], [[x[0],x[1]] for x in g.vs['ly']]))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "hnx.draw(H, with_node_labels=False, with_edge_labels=False, node_radius=.67, with_node_counts=False,\n",
    "         nodes_kwargs={'facecolors': node_colors, 'edgecolors' : 'black'},\n",
    "         edges_kwargs={'edgecolors': 'darkgrey'},\n",
    "         pos=H_ly)\n",
    "#fig.savefig('habcd_2.pdf');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot via convex hull\n",
    "H_nc = dict(zip(g.vs['name'], g.vs['color']))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "XH = xgi.Hypergraph(Edges)\n",
    "xgi.draw(XH, node_fc=H_nc, dyad_color='grey', hull=True, radius=.15, edge_fc_cmap='Greys_r', alpha=.2, pos=H_ly, node_size=8, ax=ax, node_labels=False );\n",
    "#fig.savefig('habcd_3.pdf');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge composition\n",
    "\n",
    "Recall we call a $d$-edge a **community** edge if $c>d/2$ where $c$ is the number of nodes that belong to the most represented community.\n",
    "\n",
    "Below we show the number of edges with all values $d$ and $c$, community edges or not.\n",
    "We see that given the ground-truth communities, most community edges are *pure* in the sense that $c=d$.\n",
    "\n",
    "In real examples, we usually do not know the ground-truth communities, or at least not for every node.\n",
    "We can try some clustering, for example graph clustering on the 2-section graph, or Kumar's algorithm on the hypergraph, to get a sense of edge composition.\n",
    "\n",
    "The result is quite similar to the ground-truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## edge composition - ground truth\n",
    "L = []\n",
    "for e in H.edges:\n",
    "    L.append((Counter([H_comm[i] for i in H.edges[e]]).most_common(1)[0][1],len(H.edges[e])))\n",
    "X = Counter(L).most_common()\n",
    "\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([x[0][1], x[0][0], x[0][0]>x[0][1]/2, x[1]])\n",
    "D = pd.DataFrame(np.array(L), columns=['d','c','community edge','frequency (ground truth)'])\n",
    "D = D.sort_values(by=['d','c'], ignore_index=True)\n",
    "\n",
    "## 2-section\n",
    "g.vs['leiden'] = g.community_leiden(objective_function='modularity', weights='weight').membership\n",
    "leiden = dict(zip(g.vs['name'],g.vs['leiden']))\n",
    "L = []\n",
    "for e in H.edges:\n",
    "    L.append((Counter([leiden[i] for i in H.edges[e]]).most_common(1)[0][1],len(H.edges[e])))\n",
    "X = Counter(L).most_common()\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([x[0][1], x[0][0], x[0][0]>x[0][1]/2, x[1]])\n",
    "D2 = pd.DataFrame(np.array(L), columns=['d','c','community edge','frequency (Leiden)'])\n",
    "D2 = D2.sort_values(by=['d','c'], ignore_index=True)\n",
    "\n",
    "D['frequency (Leiden)'] = D2['frequency (Leiden)']\n",
    "D = D.sort_values('frequency (ground truth)', ascending=False)\n",
    "#print(D[['d','c','frequency (ground truth)','frequency (Leiden)']].to_latex(index=False))\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simpliciality\n",
    "\n",
    "We show some measures of simpliciality, namely the number of simplicial pairs, the simpliciality matrix and the simplicial ratio measure.\n",
    "\n",
    "The simplicial ratio value is around 0.8 (recall it is based on sampling), which indicates that this hypergraph does not exhibit high simpliciality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "E = [set(H.edges[e]) for e in H.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "spl.get_simplicial_pairs(V, E, as_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spl.get_simplicial_matrix(V, E, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl.get_simplicial_measure(V, E, samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300-node noisy h-ABCD\n",
    "\n",
    "This is a noisier hypergraph with $\\xi=0.6$, edges mostly of size 4 and some edges of size 3.\n",
    "\n",
    "In the experiment below, we run each of the following algorithms 30 times and compare AMI with the ground-truth communities.\n",
    "* Leiden on 2-section (weighted) graph\n",
    "* Kumar's algorithm\n",
    "* Kumar's algorithm followed by 1 round trying to heuristically improve strict h-modularity\n",
    "\n",
    "We observe that Kumar's algorithm, which does take the hypergraph structure into account, slightly improves on the results with 2-section clustering, and heuristically improving the strict modularity further improves on the result of Kumar's algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the edges and build the h-ABCD hypergraph H\n",
    "fp = open(datadir+'ABCD/toy_300_he.txt', 'r')\n",
    "Lines = fp.readlines()\n",
    "Edges = []\n",
    "for line in Lines:\n",
    "    Edges.append(set([x for x in line.strip().split(',')]))\n",
    "H = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "\n",
    "## read the ground-truth communities and assign node colors accordingly\n",
    "H_comm = {str(k+1):v for k,v in enumerate(pd.read_csv(datadir+'ABCD/toy_300_assign.txt', header=None)[0].tolist())}\n",
    "\n",
    "## build the 2-section graph\n",
    "g = hmod.two_section(H)\n",
    "for v in g.vs:\n",
    "    v['gt'] = H_comm[v['name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce the number of repeats (REP) for a faster run (we used REP=30 for the book)\n",
    "REP = 30\n",
    "L = []\n",
    "random.seed(321)\n",
    "np.random.seed(321) \n",
    "\n",
    "for s in range(30):\n",
    "    g.vs['leiden'] = g.community_leiden(objective_function='modularity',weights='weight').membership\n",
    "    ami_g = AMI(g.vs['gt'], g.vs['leiden'])\n",
    "    H_kumar = hmod.kumar(H)\n",
    "    H_kumar_dict = hmod.part2dict(H_kumar)\n",
    "    ami_k = AMI([H_comm[v] for v in H.nodes], [H_kumar_dict[v] for v in H.nodes])\n",
    "    H_ls = hmod.part2dict(hmod.last_step(H, H_kumar, hmod.strict))\n",
    "    ami_ls = AMI([H_comm[v] for v in H.nodes], [H_ls[v] for v in H.nodes])\n",
    "    L.append([ami_g, ami_k, ami_ls])\n",
    "        \n",
    "D = pd.DataFrame(L, columns=['2-section', 'Kumar', 'h-modularity'])\n",
    "print('median values:\\n')\n",
    "print(D.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(D, width=.5, color='darkgray', linewidth=1.2)\n",
    "plt.ylabel('AMI', fontsize=14);\n",
    "#plt.savefig('habcd_cluster.eps')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no simplicial pair in this case\n",
    "E = [set(H.edges[e]) for e in H.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "spl.get_simplicial_pairs(V, E, as_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl.get_simplicial_measure(V, E, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## h-ABCD hypergraphs typically have degree-size correlation near 0\n",
    "h_deg_size_corr(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We fit two embeddings to the h-ABCD graph, namely:\n",
    "* 2-section node2vec\n",
    "* bipartite node2vec (we ignore the edge embeddings)\n",
    "\n",
    "We fit a classifier where we train on 50% of the points, and test on the rest,\n",
    "after reducing to 16-dim via UMAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section\n",
    "graph = n2v.Graph(g.to_tuple_list(), directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=32, p=1, q=1, walk_length=80, window=5, seed=123)\n",
    "nv.train(epochs=10, verbose=False)\n",
    "X_twosec = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "\n",
    "## 2-section - 2-d viz\n",
    "U = umap.UMAP().fit_transform(X_twosec)\n",
    "df = pd.DataFrame(U, columns=['X','Y'])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df.X, df.Y, c=g.vs['gt'], s=25);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bipartite (edges are in first positions; we ignore the edges)\n",
    "G = ig.Graph.from_networkx(H.bipartite())\n",
    "graph = n2v.Graph(G.to_tuple_list(), directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=32, p=1, q=1, walk_length=80, window=5, seed=123)\n",
    "nv.train(epochs=10, verbose=False)\n",
    "n_edges = len([e for e in H.edges()])\n",
    "X_bip = np.array([nv.wv[i] for i in range(len(nv.wv))])[n_edges:]\n",
    "\n",
    "## bipartite 2-d viz\n",
    "U = umap.UMAP().fit_transform(X_bip)\n",
    "df = pd.DataFrame(U,columns=['X','Y'])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df.X, df.Y, c=g.vs['gt'], s=25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit a classifier\n",
    "\n",
    "We train on half the data chosen at random, which we repeat 30 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifier - with 2-section and bipartite embeddings\n",
    "\n",
    "acc = []\n",
    "acc_b = []\n",
    "y = label = g.vs['gt']\n",
    "\n",
    "for seed in np.arange(0,301,10):\n",
    "    \n",
    "    ## 2-section\n",
    "    X = umap.UMAP(n_components=16, n_jobs=1, random_state=seed).fit_transform(X_twosec)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # print(cm)\n",
    "    acc.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "\n",
    "    ## bipartite - same seed\n",
    "    X = umap.UMAP(n_components=16, n_jobs=1, random_state=seed).fit_transform(X_bip)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # print(cm)\n",
    "    acc_b.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "    \n",
    "print(np.mean(acc), np.mean(acc_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D = pd.DataFrame(np.array([acc,acc_b]).transpose(),columns=['2-section','bipartite'])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(D, width=.5, color='darkgray', linewidth=1.2);\n",
    "plt.ylabel('Accuracy', fontsize=14);\n",
    "#plt.savefig('habcd_classify.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "The original data can be found here: https://github.com/jeffreylancaster/game-of-thrones.\n",
    "A pre-processed version is provided, where we consider an hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are named characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving each group of characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open(datadir+\"GoT/GoT.pkl\",\"rb\") as f:\n",
    "    Edges, Names, Weights = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the weighted hypergraph \n",
    "\n",
    "Use the above to build the weighted hypergraph (GoT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "GoT = hnx.Hypergraph(dict(enumerate(Edges)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add full names of characters and compute node strength (a.k.a. weighted degree)\n",
    "I, _node, _edge = GoT.incidence_matrix(index=True)\n",
    "S = I * [Weights[int(i)] for i in _edge]\n",
    "Strength = {i:j for i,j in zip(_node,S)}\n",
    "for v in GoT.nodes:\n",
    "    GoT.nodes[v].name = Names[v]\n",
    "    GoT.nodes[v].strength = Strength[v]\n",
    "for e in GoT.edges:\n",
    "    GoT.edges[e].weight = Weights[e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on the GoT hypergraph\n",
    "\n",
    "Simple exploratory data analysis (EDA) on this hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([GoT.size(e) for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge size\", fontsize=14);\n",
    "#plt.savefig('got_hist_1.eps');\n",
    "\n",
    "## max edge size\n",
    "print('max edge size:', np.max([GoT.size(e) for e in GoT.edges]))\n",
    "print('median edge size:', np.median([GoT.size(e) for e in GoT.edges]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters appearing together)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([Weights], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge weight\",fontsize=14);\n",
    "#plt.savefig('got_hist_2.eps');\n",
    "\n",
    "## max/median edge weight\n",
    "print('max edge weight:', np.max([Weights]))\n",
    "print('median edge weight:', np.median([Weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## node degrees\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(hnx.degree_dist(GoT),bins=20, color='grey')\n",
    "plt.xlabel(\"Node degree\",fontsize=14);\n",
    "#plt.savefig('got_hist_3.eps');\n",
    "\n",
    "## max degree\n",
    "print('max node degree:', np.max(hnx.degree_dist(GoT)))\n",
    "print('median node degree:', np.median(hnx.degree_dist(GoT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## node strength (total scene appearance)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([GoT.nodes[n].strength for n in GoT.nodes], bins=20, color='grey')\n",
    "plt.xlabel(\"Node strength\",fontsize=14);\n",
    "#plt.savefig('got_hist_4.eps');\n",
    "\n",
    "## max strength\n",
    "print('max node strength:', np.max([GoT.nodes[n].strength for n in GoT.nodes]))\n",
    "print('median node strength:', np.median([GoT.nodes[n].strength for n in GoT.nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a dataframe with node characteristics\n",
    "df = pd.DataFrame()\n",
    "df['name'] = [GoT.nodes[v].name for v in GoT.nodes()]\n",
    "df['degree'] = [GoT.degree(v) for v in GoT.nodes()]\n",
    "df['strength'] = [GoT.nodes[v].strength for v in GoT.nodes()]\n",
    "df.sort_values(by='strength',ascending=False).head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compute s-centrality and betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## with s=1\n",
    "bet = hnx.s_betweenness_centrality(GoT, edges=False)\n",
    "har = hnx.s_harmonic_centrality(GoT, edges=False, normalized=True)\n",
    "df['betweenness(s=1)'] = [bet[v] for v in GoT.nodes()]\n",
    "df['harmonic(s=1)'] = [har[v] for v in GoT.nodes()]\n",
    "\n",
    "## with s=2\n",
    "bet = hnx.s_betweenness_centrality(GoT, edges=False, s=2)\n",
    "har = hnx.s_harmonic_centrality(GoT, edges=False, normalized=True, s=2)\n",
    "df['betweenness(s=2)'] = [bet[v] for v in GoT.nodes()]\n",
    "df['harmonic(s=2)'] = [har[v] for v in GoT.nodes()]\n",
    "\n",
    "#print(df.sort_values(by=['strength'],ascending=False).head(10)[['name','degree','strength','betweenness(s=1)','harmonic(s=1)']].to_latex(index=False, float_format=\"{:0.5f}\".format))\n",
    "df.sort_values(by=['harmonic(s=1)'],ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures\n",
    "\n",
    "We saw several centrality measures for graphs in chapter 3. \n",
    "\n",
    "Below, we build the 2-section graph for GoT and compute a few of those. \n",
    "\n",
    "**Unlike in the first edition of the book, we now ignore edge weights to compare with the hypergraph s-measures.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "G = hmod.two_section(GoT)\n",
    "\n",
    "## betweenness\n",
    "n = G.vcount()\n",
    "b = G.betweenness(directed=False)\n",
    "G.vs['bet'] = [2*x/((n-1)*(n-2)) for x in b]\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].bet = v['bet']\n",
    "df['betweenness'] = [GoT.nodes[v].bet for v in GoT.nodes()]\n",
    "\n",
    "## harmonic\n",
    "G.vs['hc'] = G.harmonic_centrality(normalized=True)\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].hc = v['hc']\n",
    "df['harmonic'] = [GoT.nodes[v].hc for v in GoT.nodes()]\n",
    "\n",
    "## order w.r.t. harmonic\n",
    "df.sort_values(by='harmonic',ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## high correlation between centrality measures\n",
    "corr = df[['betweenness(s=1)','betweenness(s=2)','betweenness','harmonic(s=1)','harmonic(s=2)','harmonic']].corr()\n",
    "#print(corr[['harmonic','betweenness']].to_latex(index=True,  float_format=\"{:0.3f}\".format))\n",
    "print(corr[['harmonic','betweenness']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### visualize the 2-section graph\n",
    "print('nodes:',G.vcount(),'edges:',G.ecount())\n",
    "G.vs['size'] = 14\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [GoT.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 6\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "ly_fr = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout=ly_fr, bbox=(0,0,600,400), edge_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a well-separated small clique; it is the Braavosi theater troup\n",
    "print([GoT.nodes[str(x)].name for x in np.arange(166,173)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute modularity (qH) on several random partition with K parts for a range of K's\n",
    "## This should be close to 0 and can be negative.\n",
    "h = []\n",
    "for K in np.arange(2,21,2):\n",
    "    for rep in range(10):\n",
    "        V = list(GoT.nodes)\n",
    "        np.random.seed(K*rep)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = hmod.dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## drop empty sets if any\n",
    "        RandPart = [x for x in RandPart if len(x)>0]\n",
    "        ## compute qH\n",
    "        h.append(hmod.modularity(GoT, RandPart, qH2))\n",
    "print('range for qH:',min(h),'to',max(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.boxplot(h, showfliers=False, width=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the 2-section graph (with Leiden or Louvain) and compute qH\n",
    "## We now see qH >> 0\n",
    "random.seed(123)\n",
    "G.vs['leiden'] = G.community_leiden(objective_function='modularity', weights='weight').membership\n",
    "#G.vs['leiden'] = G.community_multilevel(resolution = 10, weights='weight').membership\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].leiden = v['leiden']\n",
    "df['leiden_cluster'] = [GoT.nodes[v].leiden for v in GoT.nodes()]\n",
    "ML = hmod.dict2part({v['name']:v['leiden'] for v in G.vs})\n",
    "print('qH:',\"{:.4f}\".format(hmod.modularity(GoT, ML, qH3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot 2-section w.r.t. the resulting clusters\n",
    "cl = G.vs['leiden']\n",
    "\n",
    "## pick greyscale or color plot:\n",
    "#pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "pal = ig.ClusterColoringPalette(max(cl)+2)\n",
    "\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "G.vs['label_size'] = 6\n",
    "ig.plot(G, layout = ly_fr, bbox=(0,0,600,400), edge_color='gainsboro')\n",
    "#G.vs['label_size'] = 0\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly_fr, bbox=(0,0,600,400), edge_color='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## ex: high strength nodes in same cluster with Daenerys Targaryen\n",
    "dt = df[df['name']=='Daenerys Targaryen']['leiden_cluster'].iloc[0]\n",
    "df[df['leiden_cluster']==dt].sort_values(by='strength',ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## last step heuristic given the 2-section Leiden partition\n",
    "np.random.seed(123)\n",
    "leiden = dict(zip(G.vs['name'],G.vs['leiden']))\n",
    "ls = hmod.last_step(GoT, hmod.dict2part(leiden), qH2, verbose=True)\n",
    "print('qH:',\"{:.4f}\".format(hmod.modularity(GoT, ls, qH2)))\n",
    "    \n",
    "ls_dict = hmod.part2dict(ls)\n",
    "df['cluster_qH'] = [ls_dict[v] for v in GoT.nodes()]\n",
    "G.vs['cluster_qH'] = [ls_dict[v['name']] for v in G.vs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which nodes were moved?\n",
    "df[df.leiden_cluster != df.cluster_qH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ku = hmod.kumar(GoT, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmod.modularity(GoT,Ku,qH3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot 2-section w.r.t. the resulting clusters\n",
    "cl = G.vs['cluster_qH']\n",
    "\n",
    "## pick greyscale or color plot:\n",
    "#pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "pal = ig.ClusterColoringPalette(max(cl)+2)\n",
    "\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "ig.plot(G, layout = ly_fr, bbox=(0,0,600,400), edge_color='gainsboro')\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly, bbox=(0,0,400,400))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge composition after clustering\n",
    "\n",
    "We see that the most frequent edges are small \"pure\" edges; there ar ealso several edges with all but one node from the same community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_dict = dict(zip(G.vs['name'],G.vs['cluster_qH']))\n",
    "L = []\n",
    "for e in GoT.edges:\n",
    "    L.append(tuple(x[1] for x in Counter([comm_dict[i] for i in GoT.edges[e]]).most_common()))\n",
    "X = Counter(L).most_common()\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([len(x[0]), sum(x[0]), x[0][0], x[1], x[0][0]>sum(x[0])/2])\n",
    "df_cd = pd.DataFrame(np.array(L), columns=['n_comm','d','c','frequency','community edge'],)\n",
    "df_cd['cum_freq'] = df_cd.cumsum().frequency / GoT.shape[1]\n",
    "#print(df_cd[['d','c','frequency']][:10].to_latex(index=False))\n",
    "df_cd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the overall simpliciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## compute the simplicial ratio measure\n",
    "E = [set(GoT.edges[e]) for e in GoT.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "\n",
    "## build list of edges incident to each node\n",
    "edge_dict = spl.get_edge_sets(V, E)\n",
    "\n",
    "## mapping between node index and character name\n",
    "node_dict = dict(zip([GoT.nodes[v].name for v in GoT.nodes], list(GoT.nodes)))\n",
    "\n",
    "## simplicial ratio\n",
    "spl.get_simplicial_measure(V, E, samples=100, multisets=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the individual simpliciality ratio for each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Compute the individual simpliciality ratio for each character and rank\n",
    "sm = []\n",
    "np.random.seed(123)\n",
    "for name in df.name:\n",
    "    E = edge_dict[node_dict[name]]\n",
    "    V = list(set([x for y in E for x in y]))\n",
    "    sm.append(spl.get_simplicial_measure(V, E, samples=100))\n",
    "df['simpliciality'] = sm\n",
    "df.sort_values(by='simpliciality', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick high/low simpliciality nodes - keep degree low for viz below\n",
    "hs = 'Alys Karstark'\n",
    "ls = 'Ros'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## high simpliciality\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_edge_labels':False, 'with_node_labels':False}\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "SE = [e for e in edge_dict[node_dict[hs]]]\n",
    "HG = hnx.Hypergraph(SE)\n",
    "nc = ['grey']*len(list(HG.nodes))\n",
    "idx = np.where(np.array(HG.nodes)==node_dict[hs])[0][0]\n",
    "nc[idx] = 'black'\n",
    "nr = dict(zip(HG.nodes,[1]*len(list(HG.nodes))))\n",
    "nr[node_dict[hs]] = 2\n",
    "nodes_kwargs={'facecolors':nc}\n",
    "print('looking at node:',hs)\n",
    "plt.subplots(figsize=(7,7))\n",
    "hnx.draw(HG, **kwargs, edges_kwargs=edges_kwargs, nodes_kwargs=nodes_kwargs,  node_radius=nr)\n",
    "#plt.savefig('alys.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## convex hull view\n",
    "XH = xgi.Hypergraph([list(HG.edges[e]) for e in HG.edges])\n",
    "xgi.draw(XH, node_fc='black', hull=True, node_size=[nr[i] for i in XH.nodes]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## low simpliciality\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_edge_labels':False, 'with_node_labels':False}\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "SE = [e for e in edge_dict[node_dict[ls]]]\n",
    "HG = hnx.Hypergraph(SE)\n",
    "nc = ['grey']*len(list(HG.nodes))\n",
    "idx = np.where(np.array(HG.nodes)==node_dict[ls])[0][0]\n",
    "nc[idx] = 'black'\n",
    "nr = dict(zip(HG.nodes,[1]*len(list(HG.nodes))))\n",
    "nr[node_dict[ls]] = 2\n",
    "nodes_kwargs={'facecolors':nc}\n",
    "print('looking at node:',ls)\n",
    "plt.subplots(figsize=(7,7))\n",
    "hnx.draw(HG, **kwargs, edges_kwargs=edges_kwargs, nodes_kwargs=nodes_kwargs, node_radius=nr)\n",
    "#plt.savefig('ros.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##convex hull view\n",
    "XH = xgi.Hypergraph([list(HG.edges[e]) for e in HG.edges])\n",
    "xgi.draw(XH, node_fc='black', hull=True, node_size=[nr[i] for i in XH.nodes]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 3-d view per edge size\n",
    "_, ax = plt.subplots(figsize=(10, 10), subplot_kw={\"projection\": \"3d\"})\n",
    "xgi.draw_multilayer(XH, ax=ax, node_fc='black',hull=True, node_size=[nr[i] for i in XH.nodes], sep=1, h_angle=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoT - degree - size correlation\n",
    "\n",
    "We see positive, but very small correlation in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_deg_size_corr(GoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs example \n",
    "\n",
    "Using HNX draw function to get patterns from **Figure 7.1** in the book and count motifs reported in **Table 7.2**.\n",
    "\n",
    "Given:\n",
    "* E2: number of edges of size 2\n",
    "* G(E2): graph built only with E2\n",
    "* E3: edges of size 3\n",
    "\n",
    "Compute:\n",
    "* H1: number of subgraphs of 4-nodes in G(E2) with 5 edges + 6 times the number of 4-cliques in G(E2)\n",
    "* H3: count pairs of edges in E3 with intersection of size 2\n",
    "* H2: for each (i,j,k) in E3, count common neighbours in G(E2) for (i,j), (i,k) and (j,k) \n",
    "\n",
    "Random hypergraphs:\n",
    "* probability for 2-edges: p2 = c/(n-1)\n",
    "* probability for 3-edges to maintain expected 2-section graph degree:  p3 = (8-c)/((n-1)*(n-2)) \n",
    "* probability for 3-edges to maintain expected H-degree: p3 = (8-c)/((n-1)*(n/2-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H1 pattern\n",
    "np.random.seed(1234)\n",
    "E = [{'A','B'},{'A','C'},{'A','D'},{'B','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "plt.figure(figsize=(4,4))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H2 pattern\n",
    "np.random.seed(123)\n",
    "E = [{'A','B','C'},{'A','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "plt.figure(figsize=(4,4))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H3 pattern\n",
    "np.random.seed(123)\n",
    "E = [{'A','B','C'},{'B','C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "plt.figure(figsize=(4,4))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This takes a while to run -- see some results in next cell\n",
    "choice = '2-section'\n",
    "np.random.seed(123)\n",
    "\n",
    "n = 500\n",
    "V = [str(i) for i in range(n)]\n",
    "\n",
    "L = []\n",
    "REP = 16\n",
    "\n",
    "for c in np.arange(0,9):\n",
    "    p2 = c/(n-1)\n",
    "    if choice == '2-section':\n",
    "        p3 = (8-c)/((n-1)*(n-2))    ## to maintain expected 2-section graph degree\n",
    "    else:\n",
    "        p3 = (8-c)/((n-1)*(n/2-1))  ## to maintain expected H-degree\n",
    "    print('running c =',c)\n",
    "    for rep in range(REP):\n",
    "        E2 = []\n",
    "        E3 = []\n",
    "        \n",
    "        ## generate 2-edges\n",
    "        r = np.random.random(int(n*(n-1)/2))\n",
    "        v = combinations(V,2)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p2:\n",
    "                E2.append(j)\n",
    "        ## generate 3-edges\n",
    "        r = np.random.random(int(n*(n-1)*(n-2)/6))\n",
    "        v = combinations(V,3)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p3:\n",
    "                E3.append(j)\n",
    "\n",
    "        dg = 2*len(E2)+3*len(E3)\n",
    "        HG = hnx.Hypergraph(dict(enumerate(E2+E3)))\n",
    "        g = hmod.two_section(HG)\n",
    "        sd = g.ecount()\n",
    "\n",
    "        ## count motifs in graph G with 2-edges only\n",
    "        G = ig.Graph.TupleList(E2)\n",
    "        M = G.motifs_randesu(size=4)\n",
    "        H1 = M[9] + 6*M[10] ## exactly as H1 + 6 times 4-clique\n",
    "\n",
    "        ## H2: for each 3-edge, for each pair within, count common neighbor(s) in G\n",
    "        H2 = 0\n",
    "        for e in E3:\n",
    "            if len(set(G.vs['name']).intersection(set(e)))==3:\n",
    "                s1 = set(G.neighbors(G.vs.find(name=e[0])))\n",
    "                s2 = set(G.neighbors(G.vs.find(name=e[1])))\n",
    "                s3 = set(G.neighbors(G.vs.find(name=e[2])))\n",
    "                H2 += len(s1.intersection(s2))+len(s1.intersection(s3))+len(s3.intersection(s2))\n",
    "\n",
    "        ## H3: count pairs of 3-edges with intersection of size 2\n",
    "        H3 = 0\n",
    "        e = [set(i) for i in E3]\n",
    "        l = len(e)\n",
    "        for i in np.arange(0,l-1):\n",
    "            for j in np.arange(i+1,l):\n",
    "                if len(e[i].intersection(e[j]))==2:\n",
    "                    H3+=1\n",
    "        L.append([c,H1,H2,H3,dg/n,2*sd/n])\n",
    "        \n",
    "D = pd.DataFrame(L,columns=['c','H1','H2','H3','H deg','2-sec deg'])\n",
    "D.groupby(by='c').mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### RESULTS - seed = 123 ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t16.3750\t4.086375\t8.10725\n",
    "1\t0.0000\t2.7500\t11.0000\t4.412250\t7.78375\n",
    "2\t0.0000\t10.0000\t8.4375\t4.993125\t7.89800\n",
    "3\t0.0625\t22.1250\t6.1250\t5.460875\t7.88925\n",
    "4\t0.3750\t31.2500\t3.3125\t5.961625\t7.93325\n",
    "5\t2.0000\t38.2500\t2.1875\t6.529000\t8.03175\n",
    "6\t3.3750\t38.8750\t0.9375\t7.056000\t8.04925\n",
    "7\t10.0625\t23.5625\t0.1875\t7.427000\t7.91650\n",
    "8\t15.3125\t0.0000\t0.0000\t7.932250\t7.93225\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t66.0000\t8.074125\t15.88700\n",
    "1\t0.0000\t5.0625\t50.0000\t7.867875\t14.52700\n",
    "2\t0.0000\t18.0000\t36.8125\t7.998000\t13.77350\n",
    "3\t0.0625\t44.2500\t23.1250\t7.909250\t12.69200\n",
    "4\t0.3750\t61.3125\t14.2500\t7.947625\t11.83250\n",
    "5\t2.0000\t75.1875\t9.6250\t8.009125\t10.93325\n",
    "6\t3.3750\t73.6250\t3.2500\t8.048625\t10.00225\n",
    "7\t10.0625\t46.1250\t1.0625\t7.935125\t8.91300\n",
    "8\t15.3125\t0.0000\t0.0000\t7.932250\t7.93225\n",
    "\n",
    "\n",
    "### RESULTS - no seed (1st edition) ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t15.4375\t4.037625\t8.01375\n",
    "1\t0.0000\t2.1250\t11.5000\t4.509500\t7.96650\n",
    "2\t0.0000\t11.9375\t7.1875\t5.029125\t7.97825\n",
    "3\t0.1250\t21.6875\t6.8125\t5.549375\t8.03600\n",
    "4\t0.4375\t31.4375\t3.5625\t5.967875\t7.87300\n",
    "5\t1.5000\t37.9375\t1.7500\t6.485625\t7.90475\n",
    "6\t3.5000\t35.5625\t1.4375\t6.963125\t7.92875\n",
    "7\t9.0000\t24.0000\t0.3750\t7.509375\t7.99325\n",
    "8\t15.0625\t0.0000\t0.0000\t8.012750\t8.01275\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t64.7500\t8.028750\t15.80150\n",
    "1\t0.0000\t3.9375\t47.6875\t8.091500\t14.94500\n",
    "2\t0.0625\t20.0625\t33.9375\t7.939125\t13.68550\n",
    "3\t0.0625\t43.5000\t23.6875\t7.979750\t12.81150\n",
    "4\t0.5000\t63.4375\t15.3750\t7.978500\t11.87000\n",
    "5\t1.3125\t72.5625\t9.1250\t7.951875\t10.84900\n",
    "6\t3.1250\t75.1250\t4.0000\t8.006500\t9.97400\n",
    "7\t8.3125\t52.0000\t0.6875\t8.040250\t9.04775\n",
    "8\t15.0625\t0.0000\t0.0000\t7.973000\t7.97300\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
