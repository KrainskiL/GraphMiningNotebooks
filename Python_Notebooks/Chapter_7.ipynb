{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Hypergraphs\n",
    "\n",
    "In this notebook, we introduce hypergraphs, a generalization of graphs where we allow for arbitrary sized edges (in practice, we consider edges of size 2 or more). We illustrate a few concepts using hypergraphs including modularity, community detection and transformation into 2-section graphs.\n",
    "\n",
    "This notebook was built with a previous version of the **HyperNetX** package (https://github.com/pnnl/HyperNetX).\n",
    "As of version 1.2, several functions presented here are now integrated into HyperNetX; this will be reflected in an updated version of this notebook soon.\n",
    "\n",
    "### New required package:\n",
    "\n",
    "* pip install hypernetx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set this to the data directory\n",
    "datadir='../Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import hypernetx as hnx\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from scipy.special import comb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of supplied functions for HNX hypergraphs\n",
    "\n",
    "We provide a few functions to work with hypernetx (HNX) hypergraphs. Those functions are now\n",
    "integrated into hypernetx as of version 1.2.\n",
    "\n",
    "### Build hypergraph and pre-compute key quantities\n",
    "\n",
    "We build the hypergraph HG using:\n",
    "\n",
    "```python\n",
    "HG = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "```\n",
    "\n",
    "where 'Edges' is a list of sets; edges are then indexed as 0-based integers,\n",
    "so to preserve unique ids, we represent nodes as strings.\n",
    "For example Edges[0] = {'0','2'}\n",
    "\n",
    "Once the HNX hypergraph is built, the following function is called to \n",
    "compute node strengths, d-degrees and binomial coefficients:\n",
    "\n",
    "```python\n",
    "HNX_precompute(HG)\n",
    "```\n",
    "\n",
    "### Partitions\n",
    "\n",
    "We use two representations for partitions: list of sets (the parts) or dictionary.\n",
    "Those functions are used to map from one to the other:\n",
    "\n",
    "```python\n",
    "dict2part(D)\n",
    "part2dict(A)\n",
    "```\n",
    "\n",
    "### H-modularity\n",
    "\n",
    "The function to compute H-modularity for HG w.r.t. partition A (list of sets covering the vertices):\n",
    "\n",
    "```python\n",
    "HNX_modularity(HG, A, wcd=linear)\n",
    "```\n",
    "\n",
    "where 'wcd' is the weight function (default = 'linear'). Other choices are 'strict'\n",
    "and 'majority', or any user-supplied function with the following format:\n",
    "\n",
    "```python\n",
    "def linear(d,c):\n",
    "    return c/d if c>d/2 else 0\n",
    "```\n",
    "\n",
    "where d is the edge size, and d>=c>d/2 the number of nodes in the majority class.\n",
    "\n",
    "### Two-section graph\n",
    "\n",
    "Build the random-walk based 2-section graph given some hypergraph HG:\n",
    "\n",
    "```python\n",
    "G = HNX_2section(HG)\n",
    "```\n",
    "\n",
    "where G is an igraph Graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for HNX nypergraphs as described above\n",
    "\n",
    "## Precompute some values on HNX hypergraph for computing qH faster\n",
    "def HNX_precompute(HG):\n",
    "    H = HG.remove_singletons()\n",
    "    ## 1. compute node strenghts (weighted degrees)\n",
    "    for v in H.nodes:\n",
    "        H.nodes[v].strength = 0\n",
    "    for e in H.edges:\n",
    "        try:\n",
    "            w = H.edges[e].weight\n",
    "        except:\n",
    "            w = 1\n",
    "            ## add unit weight if none to simplify other functions\n",
    "            H.edges[e].weight = 1 \n",
    "        for v in list(H.edges[e]):\n",
    "            H.nodes[v].strength += w\n",
    "    ## 2. compute d-weights        \n",
    "    ctr = Counter([len(H.edges[e]) for e in H.edges])\n",
    "    for k in ctr.keys():\n",
    "        ctr[k]=0\n",
    "    for e in H.edges:\n",
    "        ctr[len(H.edges[e])] += H.edges[e].weight\n",
    "    H.d_weights = ctr\n",
    "    H.total_weight = sum(ctr.values())\n",
    "    ## 3. compute binomial coeffcients (modularity speed-up)\n",
    "    bin_coef = {}\n",
    "    for n in H.d_weights.keys():\n",
    "        for k in np.arange(n//2+1,n+1):\n",
    "            bin_coef[(n,k)] = comb(n, k, exact=True)\n",
    "            #bin_coef[(n,k)] = factorial(n)/(factorial(k)*factorial(n-k))\n",
    "    H.bin_coef = bin_coef\n",
    "    return H\n",
    "\n",
    "#########################################\n",
    "\n",
    "## some weight function 'wdc' for d-edges with c-majority\n",
    "\n",
    "## default: linear w.r.t. c\n",
    "def linear(d,c):\n",
    "    return c/d if c>d/2 else 0\n",
    "\n",
    "## majority\n",
    "def majority(d,c):\n",
    "    return 1 if c>d/2 else 0\n",
    "\n",
    "## strict\n",
    "def strict(d,c):\n",
    "    return 1 if c==d else 0\n",
    "\n",
    "#########################################\n",
    "\n",
    "## compute vol(A_i)/vol(V) for each part A_i in A (list of sets)\n",
    "def compute_partition_probas(HG, A):\n",
    "    p = []\n",
    "    for part in A:\n",
    "        vol = 0\n",
    "        for v in part:\n",
    "            vol += HG.nodes[v].strength\n",
    "        p.append(vol)\n",
    "    s = sum(p)\n",
    "    return [i/s for i in p]\n",
    "\n",
    "## degree tax \n",
    "def DegreeTax(HG, Pr, wdc):\n",
    "    DT = 0\n",
    "    for d in HG.d_weights.keys():\n",
    "        tax = 0\n",
    "        for c in np.arange(d//2+1,d+1):\n",
    "            for p in Pr:\n",
    "                tax += p**c * (1-p)**(d-c) * HG.bin_coef[(d,c)] * wdc(d,c)\n",
    "        tax *= HG.d_weights[d]\n",
    "        DT += tax\n",
    "    DT /= HG.total_weight\n",
    "    return DT\n",
    "\n",
    "## edge contribution, A is list of sets\n",
    "def EdgeContribution(HG, A, wdc):\n",
    "    EC = 0\n",
    "    for e in HG.edges:\n",
    "        d = HG.size(e)\n",
    "        for part in A:\n",
    "            if HG.size(e,part) > d/2:\n",
    "                EC += wdc(d,HG.size(e,part)) * HG.edges[e].weight\n",
    "    EC /= HG.total_weight\n",
    "    return EC\n",
    "\n",
    "## HG: HNX hypergraph\n",
    "## A: partition (list of sets)\n",
    "## wcd: weight function (ex: strict, majority, linear)\n",
    "def HNX_modularity(HG, A, wdc=linear):\n",
    "    Pr = compute_partition_probas(HG, A)\n",
    "    return EdgeContribution(HG, A, wdc) - DegreeTax(HG, Pr, wdc)\n",
    "\n",
    "#########################################\n",
    "\n",
    "## 2-section igraph from HG\n",
    "def HNX_2section(HG):\n",
    "    s = []\n",
    "    for e in HG.edges:\n",
    "        E = HG.edges[e]\n",
    "         ## random-walk 2-section (preserve nodes' weighted degrees)\n",
    "        if len(E)>1:\n",
    "            try:\n",
    "                w = HG.edges[e].weight/(len(E)-1)\n",
    "            except:\n",
    "                w = 1/(len(E)-1)\n",
    "            s.extend([(k[0],k[1],w) for k in itertools.combinations(E,2)])\n",
    "    G = ig.Graph.TupleList(s,weights=True).simplify(combine_edges='sum')\n",
    "    return G\n",
    "\n",
    "#########################################\n",
    "\n",
    "## we use 2 representations for partitions (0-based part ids):\n",
    "## (1) dictionary or (2) list of sets\n",
    "\n",
    "def dict2part(D):\n",
    "    P = []\n",
    "    k = list(D.keys())\n",
    "    v = list(D.values())\n",
    "    for x in range(max(D.values())+1):\n",
    "        P.append(set([k[i] for i in range(len(k)) if v[i]==x]))\n",
    "    return P\n",
    "\n",
    "def part2dict(A):\n",
    "    x = []\n",
    "    for i in range(len(A)):\n",
    "        x.extend([(a,i) for a in A[i]])\n",
    "    return {k:v for k,v in x}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy hypergraph example with HNX\n",
    "\n",
    "We illustrate a few concepts with a small toy hypergraph. \n",
    "First, we build the hnx hypergraph from a list of sets (the edges), and we draw the hypergraph as well as its dual (where the role of nodes and edges are swapped).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "## using 'enumerate', edges will have integer IDs\n",
    "E = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "HD = HG.dual()\n",
    "hnx.draw(HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-computing\n",
    "\n",
    "HNX hypergraphs have node and edge weights set to 1 by default if no other values are supplied.\n",
    "The hypergraph modularity code requires a few other quantities that we pre-compute for efficiency: node strength (sum of weight of incident edges; this is the same as degree if all edge weights are equal to 1) and d-weights (sum of weights of edges of size d for each value d appearing in the hypergraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute node strength (add unit weight if none) and a few other quantities useful to quickly compute modularity\n",
    "HG = HNX_precompute(HG)\n",
    "\n",
    "## show the nodes (here strength = degree since all weights are 1 by default)\n",
    "HG.nodes.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the edges (unit weights were added by default)\n",
    "HG.edges.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d-weights distribution; here there are edges of size 2, 3 and 4 only.\n",
    "HG.d_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hypergraph modularity qH\n",
    "\n",
    "We compute qH on the toy graph for 4 different partitions, and using 3 different variations for the edge contribution.\n",
    "\n",
    "For edges of size $d$ where $c$ is the number of nodes from the part with the most representatives, we consider three variations as follows for edge contribution:\n",
    "\n",
    "* **strict**: edges are considered only if all nodes are from the same part, with unit weight, i.e. $w$ = 1 iff $c == d$ (0 else).\n",
    "* **majority**: edges are counted only if more that half the nodes are from the same part, with unit weights, i.e. $w$ = 1 iff $c>d/2$ (0 else).\n",
    "* **linear**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the number of nodes in the majority, i.e. $w = c/d$ iff $c>d/2$ (0 else).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute hypergraph modularity (qH) for the following partitions:\n",
    "A1 = [{'A','B','C'},{'D','E','F'}]           ## good clustering, qH should be positive\n",
    "A2 = [{'B','C'},{'A','D','E','F'}]           ## not so good\n",
    "A3 = [{'A','B','C','D','E','F'}]             ## this should yield qH == 0\n",
    "A4 = [{'A'},{'B'},{'C'},{'D'},{'E'},{'F'}]   ## qH should be negative here\n",
    "\n",
    "## we compute with 3 different choices of functions for the edge contribution: linear (default), strict and majority\n",
    "\n",
    "print('linear edge contribution:')\n",
    "print('qH(A1):',HNX_modularity(HG,A1),\n",
    "      'qH(A2):',HNX_modularity(HG,A2),\n",
    "      'qH(A3):',HNX_modularity(HG,A3),\n",
    "      'qH(A4):',HNX_modularity(HG,A4))\n",
    "print('strict edge contribution:')\n",
    "print('qH(A1):',HNX_modularity(HG,A1,strict),\n",
    "      'qH(A2):',HNX_modularity(HG,A2,strict),\n",
    "      'qH(A3):',HNX_modularity(HG,A3,strict),\n",
    "      'qH(A4):',HNX_modularity(HG,A4,strict))\n",
    "print('majority edge contribution:')\n",
    "print('qH(A1):',HNX_modularity(HG,A1,majority),\n",
    "      'qH(A2):',HNX_modularity(HG,A2,majority),\n",
    "      'qH(A3):',HNX_modularity(HG,A3,majority),\n",
    "      'qH(A4):',HNX_modularity(HG,A4,majority))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-section graph\n",
    "\n",
    "We build the 2-section graph for the toy hypergraph, and run graph lcustering (ECG) on this graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = HNX_2section(HG)\n",
    "G.vs['label'] = G.vs['name']\n",
    "ig.plot(G,bbox=(0,0,250,250))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section clustering with ECG\n",
    "G.vs['community'] = G.community_ecg().membership\n",
    "dict2part({v['name']:v['community'] for v in G.vs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "The original data can be found here: https://github.com/jeffreylancaster/game-of-thrones.\n",
    "A pre-processed version is provided, where we consider an hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are named characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving each group of characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open(datadir+\"GoT/GoT.pkl\",\"rb\") as f:\n",
    "    Edges, Names, Weights = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build weighted hypergraph \n",
    "\n",
    "Use the above to build the weighted hypergraph (GoT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "GoT = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "\n",
    "## add edge weights\n",
    "for e in GoT.edges:\n",
    "    GoT.edges[e].weight = Weights[e]\n",
    "\n",
    "## add full names of characters\n",
    "for v in GoT.nodes:\n",
    "    GoT.nodes[v].name = Names[v]\n",
    "\n",
    "## pre-compute required quantities for modularity and clustering\n",
    "GoT = HNX_precompute(GoT)\n",
    "\n",
    "print(GoT.number_of_nodes(),'nodes and',GoT.number_of_edges(),'edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of a node (indices are strings)\n",
    "GoT.nodes['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of an edge (indices are integers)\n",
    "GoT.edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get the nodes for a given edge\n",
    "GoT.edges[0].elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or just the keys\n",
    "GoT.edges[0].elements.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on GoT hypergraph\n",
    "\n",
    "Simple exploratory data analysis (EDA) on this hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "plt.hist([GoT.edges[e].size() for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge size\",fontsize=14);\n",
    "#plt.savefig('got_hist_1.eps');\n",
    "## max edge size\n",
    "print('max = ',max([GoT.edges[e].size() for e in GoT.edges]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters appearing together)\n",
    "plt.hist([GoT.edges[e].weight for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge weight\",fontsize=14);\n",
    "#plt.savefig('got_hist_2.eps');\n",
    "## max edge weight\n",
    "print('max = ',max([GoT.edges[e].weight for e in GoT.edges]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## node degrees\n",
    "plt.hist(hnx.degree_dist(GoT),bins=20, color='grey')\n",
    "plt.xlabel(\"Node degree\",fontsize=14);\n",
    "#plt.savefig('got_hist_3.eps');\n",
    "## max degree\n",
    "print('max = ',max(hnx.degree_dist(GoT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node strength (total appearance)\n",
    "plt.hist([GoT.nodes[n].strength for n in GoT.nodes], bins=20, color='grey')\n",
    "plt.xlabel(\"Node strength\",fontsize=14);\n",
    "#plt.savefig('got_hist_4.eps');\n",
    "## max strength\n",
    "print('max = ',max([GoT.nodes[n].strength for n in GoT.nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a dataframe with node characteristics\n",
    "dg = [GoT.degree(v) for v in GoT.nodes()]\n",
    "st = [GoT.nodes[v].strength for v in GoT.nodes()]\n",
    "nm = [GoT.nodes[v].name for v in GoT.nodes()]\n",
    "D = pd.DataFrame(np.array([nm,dg,st]).transpose(),columns=['name','degree','strength'])\n",
    "D['degree'] = pd.to_numeric(D['degree'])\n",
    "D['strength'] = pd.to_numeric(D['strength'])\n",
    "\n",
    "## sort w.r.t. strength\n",
    "D.sort_values(by='strength',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort w.r.t. degree\n",
    "D.sort_values(by='degree',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## we see clear correlation between degree and strength\n",
    "plt.plot(D['degree'],D['strength'],'.')\n",
    "plt.xlabel('degree',fontsize=14)\n",
    "plt.ylabel('strength',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures\n",
    "\n",
    "We saw several centrality measures for graphs in chapter 3. Below, we build the 2-section graph for GoT and compute a few of those.\n",
    "\n",
    "Node ordering should be preserved and we verify that it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "G = HNX_2section(GoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check -- node ordering is the same in GoT and G\n",
    "\n",
    "## ordering of nodes in GoT\n",
    "ord_GoT = list(GoT.nodes.elements.keys())\n",
    "\n",
    "## ordering of nodes in G\n",
    "ord_G = [v['name'] for v in G.vs]\n",
    "\n",
    "ord_GoT == ord_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = G.betweenness(directed=False,weights='weight')\n",
    "n = G.vcount()\n",
    "D['betweenness'] = [2*x/((n-1)*(n-2)) for x in b]\n",
    "D['pagerank'] = G.pagerank(directed=False,weights='weight')\n",
    "\n",
    "## order w.r.t. betweenness\n",
    "D.sort_values(by='betweenness',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## order w.r.t. pagerank\n",
    "D.sort_values(by='pagerank',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the 2-section graph\n",
    "print('nodes:',G.vcount(),'edges:',G.ecount())\n",
    "G.vs['size'] = 10\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [GoT.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 5\n",
    "ly = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout = ly, bbox=(0,0,600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a well-separated small clique; it is the Braavosi theater troup\n",
    "print([GoT.nodes[str(x)].name for x in np.arange(166,173)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute modularity (qH) on several random partition with K parts for a range of K's\n",
    "## This should be close to 0 and can be negative.\n",
    "h = []\n",
    "for K in np.arange(2,21):\n",
    "    for rep in range(10):\n",
    "        V = list(GoT.nodes)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## drop empty sets if any\n",
    "        RandPart = [x for x in RandPart if len(x)>0]\n",
    "        ## compute qH\n",
    "        h.append(HNX_modularity(GoT, RandPart))\n",
    "print('range for qH:',min(h),'to',max(h))\n",
    "plt.boxplot(h, showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the 2-section graph (with Louvain) and compute qH\n",
    "## We now see qH >> 0\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "D['cluster'] = G.vs['louvain']\n",
    "ML = dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "## Compute qH\n",
    "print(HNX_modularity(GoT, ML))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot 2-section w.r.t. the resulting clusters\n",
    "cl = G.vs['louvain']\n",
    "\n",
    "## pick greyscale or color plot:\n",
    "#pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "pal = ig.ClusterColoringPalette(max(cl)+1)\n",
    "\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "G.vs['label_size'] = 5\n",
    "ig.plot(G, layout = ly, bbox=(0,0,500,400))\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly, bbox=(0,0,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex: high strength nodes in same cluster with Daenerys Targaryen\n",
    "dt = int(D[D['name']=='Daenerys Targaryen']['cluster'])\n",
    "D[D['cluster']==dt].sort_values(by='strength',ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with simple random hypergraphs with communities\n",
    "\n",
    "Note: qH-based heuristics are still very experimental; we only provide this for illustration in **Section 7.4** of the book. Experiment results are stored in files taus_xx.pkl with xx in {00, 05, 10, 15}.\n",
    "\n",
    "For each experiment, we have results for:\n",
    "\n",
    "* 16 hypergraphs each with 1000 nodes, 1400 edges of size 2 to 8 (200 each)\n",
    "* 10 communities with 0%, 5%, 10% or 15% of \"noise\" edges ($\\mu$)\n",
    "* community edge homogeneity ($\\tau$) from 0.5 to 1\n",
    "* communities obtained via 3 algorithms:\n",
    " * qG-based Louvain on the 2-section graph\n",
    " * qH-based heuristic clustering algorithm on the hypergraph\n",
    " * qH+: same but using true homogeneity ($\\tau$)\n",
    "\n",
    "Recall that AMI = adjusted mutual information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results (here mu = .05) and plot\n",
    "with open( datadir+\"Hypergraph/taus_05.pkl\", \"rb\" ) as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "R = pd.DataFrame(results,columns=['tau','Graph','Hypergraph','Hypergraph+']).groupby(by='tau').mean()\n",
    "t = [x for x in np.arange(.501,1,.025)]\n",
    "\n",
    "## color or greyscale\n",
    "pal = ig.GradientPalette(\"grey\",\"black\",3)\n",
    "#pal = ig.GradientPalette(\"red\",\"blue\",3)\n",
    "\n",
    "## plot\n",
    "plt.plot(t,R['Graph'],'o-',label='qG-based',color=pal[0])\n",
    "plt.plot(t,R['Hypergraph'],'o-',label='qH-based',color=pal[1])\n",
    "plt.plot(t,R['Hypergraph+'],'o-',label='qH-based (tuned)',color=pal[2])\n",
    "plt.xlabel(r'homogeneity ($\\tau$)',fontsize=14)\n",
    "plt.ylabel('AMI',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('taus_05.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community hypergraphs\n",
    "\n",
    "We provide hyperedge list and communities for 3 random hypergraph with communities, namely:\n",
    "\n",
    "* edges_65, comm_65: hypergraph with $\\tau_e = \\lceil(d*0.65)\\rceil$ for all community edges of size $d$\n",
    "* edges_85, comm_85: hypergraph with $\\tau_e = \\lceil(d*0.85)\\rceil$ for all community edges of size $d$\n",
    "* edges_65_unif, comm_65_unif: hypergraph with $\\tau_e$ chosen uniformly from $\\{\\lceil(d*0.65)\\rceil,...,d\\}$ for all community edges of size $d$\n",
    "\n",
    "All have 1000 nodes, 1400 edges of size 2 to 8 (200 each) 10 communities and noise parameter $\\mu=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the edge lists and communities\n",
    "with open(datadir+\"Hypergraph/hypergraphs.pkl\",\"rb\") as f:\n",
    "    (edges_65, comm_65, edges_85, comm_85, edges_65_unif, comm_65_unif) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiment below, we estimate the homogeneity parameter $\\tau$ via clustering on the 2-section graph and comare with the results we get using the true communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick one of the three hypergraphs\n",
    "comm = comm_65\n",
    "L = edges_65\n",
    "\n",
    "## build hypergraph\n",
    "HG = hnx.Hypergraph(dict(enumerate(L)))\n",
    "\n",
    "## compute P(homogeneity > $\\tau$) using the true communities\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm])/len(e))\n",
    "y = []\n",
    "for t in np.arange(.501,1,.025):\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "plt.plot(np.arange(.501,1,.025),y,'.-',color='grey',label='true communities')\n",
    "\n",
    "## same but using the communities obtained via Louvain algorithm on the 2-section graph\n",
    "G = HNX_2section(HG)\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "ML = dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in ML])/len(e))\n",
    "y = []\n",
    "for t in np.arange(.501,1,.025):\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "plt.plot(np.arange(.501,1,.025),y,'.-',color='black',label='Louvain')\n",
    "\n",
    "## add grid and legend\n",
    "plt.grid()\n",
    "#plt.title(r'Estimating $\\tau$ from data',fontsize=14)\n",
    "plt.ylabel(r'Pr(homogeneity > $\\tau$)',fontsize=14)\n",
    "plt.xlabel(r'$\\tau$',fontsize=14)\n",
    "plt.legend()\n",
    "plt.ylim(0,1);\n",
    "#plt.savefig('tau_65.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity with all tau = 0.65\n",
    "## results vary in view of various edge sizes, nd some \"noise\" edges.\n",
    "x = []\n",
    "for e in edges_65:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm_65])/len(e))\n",
    "plt.hist(x,bins='rice',color='grey');\n",
    "#plt.savefig('hist_65.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity with tau varying from 0.65 to 1\n",
    "## we see many more pure community edges in this case, as expected\n",
    "x = []\n",
    "for e in edges_65_unif:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm_65_unif])/len(e))\n",
    "plt.hist(x, bins='rice',color='grey');\n",
    "#plt.savefig('hist_65_unif.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs example \n",
    "\n",
    "Using HNX draw function to get patterns from Figure 7.1 in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H1 pattern\n",
    "E = [{'A','B'},{'A','C'},{'A','D'},{'B','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H2 pattern\n",
    "E = [{'A','B','C'},{'A','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H3 pattern\n",
    "E = [{'A','B','C'},{'B','C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Counting those patterns -- Table 7.2 in the book"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This takes several minutes -- see results in next cell\n",
    "from itertools import combinations\n",
    "\n",
    "## given:\n",
    "## E2: edges of size 2\n",
    "## G(E2): graph built only with E2\n",
    "## E3: edges of size 3\n",
    "## compute:\n",
    "## H1: number of subgraphs of 4-nodes in G(E2) with 5 edges + 6 times 4-cliques in G(E2)\n",
    "## H3: count pairs of edges in E3 with intersection of size 2\n",
    "## H2: for each (i,j,k) in E3, count common neighbours in G(E2) for (i,j), (i,k) and (j,k) \n",
    "\n",
    "n = 500\n",
    "V = [str(i) for i in range(n)]\n",
    "\n",
    "L = []\n",
    "REP = 16\n",
    "\n",
    "for c in np.arange(0,9):\n",
    "    p2 = c/(n-1)\n",
    "    p3 = (8-c)/((n-1)*(n-2))    ## maintain expected 2-section graph degree\n",
    "    #p3 = (8-c)/((n-1)*(n/2-1)) ## maintain expected H-degree\n",
    "    print(c)\n",
    "    for rep in range(REP):\n",
    "        E2 = []\n",
    "        E3 = []\n",
    "        \n",
    "        ## generate 2-edges\n",
    "        r = np.random.random(int(n*(n-1)/2))\n",
    "        v = combinations(V,2)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p2:\n",
    "                E2.append(j)\n",
    "        ## generate 3-edges\n",
    "        r = np.random.random(int(n*(n-1)*(n-2)/6))\n",
    "        v = combinations(V,3)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p3:\n",
    "                E3.append(j)\n",
    "\n",
    "        dg = 2*len(E2)+3*len(E3)\n",
    "        HG = hnx.Hypergraph(dict(enumerate(E2+E3)))\n",
    "        g = HNX_2section(HG)\n",
    "        sd = g.ecount()\n",
    "\n",
    "        ## count motifs in graph G with 2-edges only\n",
    "        G = ig.Graph.TupleList(E2)\n",
    "        M = G.motifs_randesu(size=4)\n",
    "        H1 = M[9] + 6*M[10] ## exactly as H1 + 6 times 4-clique\n",
    "\n",
    "        ## H2: for each 3-edge, for each pair within, count common neighbor(s) in G\n",
    "        H2 = 0\n",
    "        for e in E3:\n",
    "            if len(set(G.vs['name']).intersection(set(e)))==3:\n",
    "                s1 = set(G.neighbors(G.vs.find(name=e[0])))\n",
    "                s2 = set(G.neighbors(G.vs.find(name=e[1])))\n",
    "                s3 = set(G.neighbors(G.vs.find(name=e[2])))\n",
    "                H2 += len(s1.intersection(s2))+len(s1.intersection(s3))+len(s3.intersection(s2))\n",
    "\n",
    "        ## H3: count pairs of 3-edges with intersection of size 2\n",
    "        H3 = 0\n",
    "        e = [set(i) for i in E3]\n",
    "        l = len(e)\n",
    "        for i in np.arange(0,l-1):\n",
    "            for j in np.arange(i+1,l):\n",
    "                if len(e[i].intersection(e[j]))==2:\n",
    "                    H3+=1\n",
    "        L.append([c,H1,H2,H3,dg/n,2*sd/n])\n",
    "        \n",
    "D = pd.DataFrame(L,columns=['c','H1','H2','H3','H deg','2-sec deg'])\n",
    "D.groupby(by='c').mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### RESULTS ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t15.4375\t4.037625\t8.01375\n",
    "1\t0.0000\t2.1250\t11.5000\t4.509500\t7.96650\n",
    "2\t0.0000\t11.9375\t7.1875\t5.029125\t7.97825\n",
    "3\t0.1250\t21.6875\t6.8125\t5.549375\t8.03600\n",
    "4\t0.4375\t31.4375\t3.5625\t5.967875\t7.87300\n",
    "5\t1.5000\t37.9375\t1.7500\t6.485625\t7.90475\n",
    "6\t3.5000\t35.5625\t1.4375\t6.963125\t7.92875\n",
    "7\t9.0000\t24.0000\t0.3750\t7.509375\t7.99325\n",
    "8\t15.0625\t0.0000\t0.0000\t8.012750\t8.01275\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t64.7500\t8.028750\t15.80150\n",
    "1\t0.0000\t3.9375\t47.6875\t8.091500\t14.94500\n",
    "2\t0.0625\t20.0625\t33.9375\t7.939125\t13.68550\n",
    "3\t0.0625\t43.5000\t23.6875\t7.979750\t12.81150\n",
    "4\t0.5000\t63.4375\t15.3750\t7.978500\t11.87000\n",
    "5\t1.3125\t72.5625\t9.1250\t7.951875\t10.84900\n",
    "6\t3.1250\t75.1250\t4.0000\t8.006500\t9.97400\n",
    "7\t8.3125\t52.0000\t0.6875\t8.040250\t9.04775\n",
    "8\t15.0625\t0.0000\t0.0000\t7.973000\t7.97300\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
