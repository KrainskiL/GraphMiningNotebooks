{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- install node2vec(*) code and add executable to your $PATH (code: https://snap.stanford.edu/node2vec)\n",
    "- compile GED code (graph embedding divergence), \n",
    "  the base implementation of the framework in C (the code is included, and can also be found at      https://github.com/ftheberge/Comparing_Graph_Embeddings) \n",
    "- new package to install: 'pip install --no-dependencies graphrole'\n",
    "- adjust location of data and code in next cell\n",
    "\n",
    "(*) With the node2vec code, we got some unstable results on some older Linux platforms. In such cases, one possible way to improve the results is to reduce the path length for the random walks (default = 80) to something smaller like 10 or 20. The option is \"-l:10\" or \"-l:20\" respectively. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data directory\n",
    "datadir='../Datasets/'\n",
    "\n",
    "## location of the GED code\n",
    "GED='../GED/GED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import os\n",
    "import umap\n",
    "import pickle\n",
    "import partition_igraph\n",
    "import subprocess\n",
    "import scipy.sparse.linalg as lg\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "from graphrole import RecursiveFeatureExtractor, RoleExtractor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import calinski_harabasz_score as CHS\n",
    "from sklearn.metrics import silhouette_score as SIL\n",
    "#%config Completer.use_jedi = False\n",
    "\n",
    "## node and edge greyscale colors\n",
    "cls_edges = 'gainsboro'\n",
    "cls = ['silver','dimgray','black']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as defined in the node2vec paper\n",
    "def binary_operator(u, v, op='had'):\n",
    "    if op=='had':\n",
    "        return u * v\n",
    "    if op=='l1':\n",
    "        return np.abs(u - v)\n",
    "    if op=='l2':\n",
    "        return (u - v) ** 2\n",
    "    if op=='avg':\n",
    "        return (u + v) / 2.0\n",
    "    \n",
    "def readEmbedding(fn=\"_embed\", N2K=None):\n",
    "    D = pd.read_csv(fn, sep=' ', skiprows=1, header=None)\n",
    "    D = D.dropna(axis=1)\n",
    "    if N2K!=None:\n",
    "        x = [N2K[i] for i in D[0]]\n",
    "        D[0] = x    \n",
    "        D = D.sort_values(by=0)\n",
    "    Y = np.array(D.iloc[:,1:])\n",
    "    return Y\n",
    "\n",
    "## Read embedding from file in node2vec format\n",
    "## Map to layout format\n",
    "## for visualization, we use UMAP if dim > 2\n",
    "def embed2layout(fn=\"_embed\"):\n",
    "    D = pd.read_csv(fn, sep=' ', skiprows=1, header=None)\n",
    "    D = D.dropna(axis=1)\n",
    "    D = D.sort_values(by=0)\n",
    "    Y = np.array(D.iloc[:,1:])\n",
    "    if Y.shape[1]>2:\n",
    "        Y = umap.UMAP().fit_transform(Y)\n",
    "    ly = []\n",
    "    for v in range(Y.shape[0]):\n",
    "        ly.append((Y[v][0],Y[v][1]))\n",
    "    return ly\n",
    "\n",
    "\n",
    "## Computing JS divergence with GED code given edgelist, communities and embedding\n",
    "def JS(edge_file, comm_file, embed_file, entropy=False):\n",
    "    if entropy:\n",
    "        x = GED+' -E -g '+edge_file+' -c '+comm_file+' -e '+embed_file\n",
    "    else:\n",
    "        x = GED+' -g '+edge_file+' -c '+comm_file+' -e '+embed_file\n",
    "    s = subprocess.run(x, shell=True, stdout=subprocess.PIPE)\n",
    "    x = s.stdout.decode().split(' ')\n",
    "    div = float(x[1])\n",
    "    return(div)\n",
    "\n",
    "\n",
    "## Hope embedding with various similarity functions\n",
    "def Hope(g, sim='katz', dim=2, verbose=False, beta=.01, alpha=.5):\n",
    "    ## For undirected graphs, embedding as source and target are identical\n",
    "    if g.is_directed() == False:\n",
    "        dim = dim*2\n",
    "    A = np.array(g.get_adjacency().data)\n",
    "    beta = beta\n",
    "    alpha = alpha\n",
    "    n = g.vcount()\n",
    "    ## Katz\n",
    "    if sim == 'katz':\n",
    "        M_g = np.eye(n) - beta * A\n",
    "        M_l = beta * A\n",
    "    ## Adamic-Adar\n",
    "    if sim == 'aa':\n",
    "        M_g = np.eye(n)\n",
    "        ## fix bug 1/x and take log();\n",
    "        D = np.diag([1/np.log(x) if x>1 else 0 for x in g.degree()]) \n",
    "        # D = np.diag([1/np.log(max(2,x)) for x in g.degree()]) \n",
    "        M_l = np.dot(np.dot(A,D),A)\n",
    "        np.fill_diagonal(M_l,0)\n",
    "    ## Common neighbors\n",
    "    if sim == 'cn':\n",
    "        M_g = np.eye(n)\n",
    "        M_l = np.dot(A,A)\n",
    "    ## presonalized page rank\n",
    "    if sim == 'ppr':\n",
    "        P = []\n",
    "        for i in range(n):\n",
    "            s = np.sum(A[i])\n",
    "            if s>0:\n",
    "                P.append([x/s for x in A[i]])\n",
    "            else:\n",
    "                P.append([1/n for x in A[i]])\n",
    "        P = np.transpose(np.array(P)) ## fix bug - take transpose\n",
    "        M_g = np.eye(n)-alpha*P\n",
    "        M_l = (1-alpha)*np.eye(n)\n",
    "    S = np.dot(np.linalg.inv(M_g), M_l)\n",
    "    u, s, vt = lg.svds(S, k=dim // 2)\n",
    "    X1 = np.dot(u, np.diag(np.sqrt(s)))\n",
    "    X2 = np.dot(vt.T, np.diag(np.sqrt(s)))\n",
    "    X = np.concatenate((X1, X2), axis=1)\n",
    "    p_d_p_t = np.dot(u, np.dot(np.diag(s), vt))\n",
    "    eig_err = np.linalg.norm(p_d_p_t - S)\n",
    "    if verbose:\n",
    "        print('SVD error (low rank): %f' % eig_err)\n",
    "    ## undirected graphs have identical source and target embeddings\n",
    "    if g.is_directed() == False:\n",
    "        d = dim//2\n",
    "        return X[:,:d]\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "## save to disk to compute divergence\n",
    "def saveEmbedding(X, g, fn='_embed'):\n",
    "    with open(fn,'w') as f:\n",
    "        f.write(str(X.shape[0]) + \" \" + str(X.shape[1])+'\\n')\n",
    "        for i in range(X.shape[0]):\n",
    "            f.write(g.vs[i]['name']+' ')\n",
    "            for j in range(X.shape[1]):\n",
    "                f.write(str(X[i][j])+' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "## Laplacian eigenmaps embedding\n",
    "def LE(g, dim=2):\n",
    "    L_sym = np.array(g.laplacian(normalized=True))\n",
    "    w, v = lg.eigs(L_sym, k=dim + 1, which='SM')\n",
    "    idx = np.argsort(w) # sort eigenvalues\n",
    "    w = w[idx]\n",
    "    v = v[:, idx]\n",
    "    X = v[:, 1:]\n",
    "    return X.real\n",
    "\n",
    "def bmatrix(a):\n",
    "    \"\"\"Returns a LaTeX bmatrix\n",
    "\n",
    "    :a: numpy array\n",
    "    :returns: LaTeX bmatrix as a string\n",
    "    \"\"\"\n",
    "    if len(a.shape) > 2:\n",
    "        raise ValueError('bmatrix can at most display two dimensions')\n",
    "    lines = str(a).replace('[', '').replace(']', '').splitlines()\n",
    "    rv = [r'\\begin{bmatrix}']\n",
    "    rv += ['  ' + ' & '.join(l.split()) + r'\\\\' for l in lines]\n",
    "    rv +=  [r'\\end{bmatrix}']\n",
    "    return '\\n'.join(rv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To illustrate random walks\n",
    "g = ig.Graph.Erdos_Renyi(n=4,p=0,directed=True)\n",
    "g.vs['label'] = ['A','B','C','D']\n",
    "g.vs['color'] = 'white'\n",
    "g.add_edges([(0,1),(1,2),(1,3),(2,1),(3,2)])\n",
    "#ig.plot(g,'tiny.eps',bbox=(0,0,300,200),vertex_label_size=10)\n",
    "ig.plot(g,bbox=(0,0,300,200),vertex_label_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare datasets\n",
    "\n",
    "* g: small ABCD graph (100 nodes), mainly for visualization and quick exampes\n",
    "* G: larger ABCD graph (1000 nodes), for experiments\n",
    "* z: zachary graph, for visualzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Small ABCD graph "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ABCD graph -- small enough for viz\n",
    "## We used the following parameters:\n",
    "n = \"100\"                     # number of vertices in graph\n",
    "t1 = \"3\"                      # power-law exponent for degree distribution\n",
    "d_min = \"5\"                   # minimum degree\n",
    "d_max = \"15\"                  # maximum degree\n",
    "d_max_iter = \"1000\"           # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                      # power-law exponent for cluster size distribution\n",
    "c_min = \"25\"                  # minimum cluster size\n",
    "c_max = \"50\"                  # maximum cluster size\n",
    "c_max_iter = \"1000\"           # maximum number of iterations for sampling cluster sizes\n",
    "xi = \"0.2\"                    # fraction of edges to fall in background graph\n",
    "isCL = \"false\"                # if \"false\" use configuration model, if \"true\" use Chung-Lu\n",
    "degreefile = \"degrees.dat\"               # name of file that contains vertex degrees\n",
    "communitysizesfile = \"comm_sizes.dat\"    # name of file that contains community sizes\n",
    "communityfile = \"abcd_100_comm.dat\" # name of file that contains assignments of vertices to communities\n",
    "networkfile = \"abcd_100.dat\"        # name of file that contains edges of the generated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "g = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_100.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_100_comms.dat',dtype='uint16',usecols=(1))\n",
    "g.vs['comm'] = [c[int(x['name'])-1] for x in g.vs]\n",
    "## print a few stats\n",
    "print(g.vcount(),'vertices,',g.ecount(),'edges,','avg degreee',np.mean(g.degree()),'communities',max(g.vs['comm']))\n",
    "## ground truth\n",
    "gt = {k:(v-1) for k,v in enumerate(g.vs['comm'])}\n",
    "## map between int(name) to key\n",
    "n2k = {int(v):k for k,v in enumerate(g.vs['name'])}\n",
    "\n",
    "## define the colors and node sizes here\n",
    "g.vs['size'] = 7\n",
    "g.es['color'] = cls_edges\n",
    "g.vs['color'] = [cls[i-1] for i in g.vs['comm']]\n",
    "#ig.plot(g, 'abcd.eps', bbox=(0,0,300,200))\n",
    "ig.plot(g, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Larger, noisy ABCD graph\n",
    "\n",
    "This is a larger graph with lots of noise edges (xi=0.6)\n",
    "\n",
    "We'll use a version with stronger communities (xi=0.2) for link prediction.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ABCD graph -- larger for experiments\n",
    "## We used the following parameters:\n",
    "n = \"1000\"                     # number of vertices in graph\n",
    "t1 = \"3\"                       # power-law exponent for degree distribution\n",
    "d_min = \"10\"                   # minimum degree\n",
    "d_max = \"100\"                  # maximum degree\n",
    "d_max_iter = \"1000\"            # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                       # power-law exponent for cluster size distribution\n",
    "c_min = \"50\"                   # minimum cluster size\n",
    "c_max = \"150\"                  # maximum cluster size\n",
    "c_max_iter = \"1000\"            # maximum number of iterations for sampling cluster sizes\n",
    "xi = \"0.6\"                     # fraction of edges to fall in background graph\n",
    "isCL = \"false\"                 # if \"false\" use configuration model, if \"true\" use Chung-Lu\n",
    "degreefile = \"degrees.dat\"               # name of file that contains vertex degrees\n",
    "communitysizesfile = \"comm_sizes.dat\"    # name of file that contains community sizes\n",
    "communityfile = \"abcd_1000_comm.dat\"       # name of file that contains assignments of vertices to communities\n",
    "networkfile = \"abcd_1000.dat\"              # name of file that contains edges of the generated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "G = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_1000.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_1000_comms.dat',dtype='uint16',usecols=(1))\n",
    "G.vs['comm'] = [c[int(x['name'])-1] for x in G.vs]\n",
    "## print a few stats\n",
    "print(G.vcount(),'vertices,',G.ecount(),'edges,','avg degreee',np.mean(G.degree()),'communities',max(G.vs['comm']))\n",
    "## ground truth\n",
    "GT = {k:(v-1) for k,v in enumerate(G.vs['comm'])}\n",
    "## map between int(name) to key\n",
    "N2K = {int(v):k for k,v in enumerate(G.vs['name'])}\n",
    "## define the colors and node sizes here\n",
    "cls_edges = 'gainsboro'\n",
    "G.vs['size'] = 5\n",
    "G.es['color'] = cls_edges\n",
    "G.vs['color'] = 'black'\n",
    "ig.plot(G, bbox=(0,0,400,300)) ## communities are far from obvious in 2d layout!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zachary (karate) graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ig.Graph.Famous('zachary')\n",
    "z.vs['size'] = 7\n",
    "z.vs['name'] = [str(i) for i in range(z.vcount())]\n",
    "z.es['color'] = cls_edges\n",
    "z.vs['comm'] = [0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "z.vs['color'] = [cls[i*2] for i in z.vs['comm']]\n",
    "#ig.plot(z, 'zachary.eps', bbox=(0,0,300,200))\n",
    "ig.plot(z, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show various 2d layouts using small Zachary graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('kk')\n",
    "#ig.plot(z, 'layout_kk.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('fr')\n",
    "#ig.plot(z, 'layout_fr.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('mds')\n",
    "#ig.plot(z, 'layout_mds.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('circle')\n",
    "#ig.plot(z, 'layout_circle.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('grid')\n",
    "#ig.plot(z, 'layout_grid.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly = z.layout('sugiyama')\n",
    "#ig.plot(z, 'layout_tree.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(z, layout=ly, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform several embeddings -- Zachary graph\n",
    "* node2vec from source code\n",
    "* HOPE with different similarities\n",
    "* Laplacian Eigenmaps\n",
    "* visualize some good and bad results\n",
    "\n",
    "We use the framework to compute the \"graph embedding divergence\" (GED.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "DIM = [5,10,15]\n",
    "best_jsd = 1\n",
    "worst_jsd = 0\n",
    "\n",
    "## Hope\n",
    "for dim in DIM:\n",
    "    for sim in ['katz','ppr','cn','aa']:\n",
    "        X = Hope(z,sim=sim,dim=dim) \n",
    "        saveEmbedding(X,z)\n",
    "        jsd = JS(datadir+'Zachary/zachary.edgelist',datadir+'Zachary/zachary.ecg','_embed')        \n",
    "        ## keep track of best and worst\n",
    "        if jsd < best_jsd:\n",
    "            os.system('cp _embed _embed_best')\n",
    "            best_jsd = jsd\n",
    "        if jsd > worst_jsd:\n",
    "            os.system('cp _embed _embed_worst')\n",
    "            worst_jsd = jsd\n",
    "        L.append([dim,'hope',sim,jsd])\n",
    "\n",
    "## LE\n",
    "for dim in DIM:\n",
    "    X = LE(z,dim=dim)\n",
    "    saveEmbedding(X,z)\n",
    "    jsd = JS(datadir+'Zachary/zachary.edgelist',datadir+'Zachary/zachary.ecg','_embed')\n",
    "    ## keep track of best and worst\n",
    "    if jsd < best_jsd:\n",
    "        os.system('cp _embed _embed_best')\n",
    "        best_jsd = jsd\n",
    "    if jsd > worst_jsd:\n",
    "        os.system('cp _embed _embed_worst')\n",
    "        worst_jsd = jsd\n",
    "    L.append([dim,'le',' ',jsd])\n",
    "    \n",
    "## node2vec is in my path\n",
    "for dim in DIM:\n",
    "    for (p,q) in [(1,0.1),(0.1,1),(1,1)]:\n",
    "        x = 'node2vec -i:'+datadir+'Zachary/zachary.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        ## if you get unstable results, you can try with shorter random walks, for example:\n",
    "        ## x = 'node2vec -l:3 -i:'+datadir+'Zachary/zachary.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        r = os.system(x)\n",
    "        jsd = JS(datadir+'Zachary/zachary.edgelist',datadir+'Zachary/zachary.ecg','_embed')\n",
    "        ## keep track of best and worst\n",
    "        if jsd < best_jsd:\n",
    "            os.system('cp _embed _embed_best')\n",
    "            best_jsd = jsd\n",
    "        if jsd > worst_jsd:\n",
    "            os.system('cp _embed _embed_worst')\n",
    "            worst_jsd = jsd\n",
    "\n",
    "        L.append([dim,'n2v',str(p)+' '+str(q),jsd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.DataFrame(L,columns=['dim','algo','param','jsd'])\n",
    "D = D.sort_values(by='jsd',axis=0)\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cp _embed_best _embed')\n",
    "l = embed2layout()\n",
    "z.vs['ly'] = [l[int(v['name'])] for v in z.vs]\n",
    "#ig.plot(z, 'zac_high.eps', layout=z.vs['ly'], bbox=(0,0,300,200))\n",
    "ig.plot(z,layout=z.vs['ly'], bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cp _embed_worst _embed')\n",
    "l = embed2layout()\n",
    "z.vs['ly'] = [l[int(v['name'])] for v in z.vs]\n",
    "#ig.plot(z, 'zac_high.eps', layout=z.vs['ly'], bbox=(0,0,300,200))\n",
    "ig.plot(z,layout=z.vs['ly'], bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform several embeddings -- small ABCD  graph\n",
    "* node2vec from source code\n",
    "* HOPE different similarities\n",
    "* Laplacian Eigenmaps\n",
    "* visualize some good and bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "DIM = [2,4,8,16,24,32]\n",
    "best_jsd = 1\n",
    "worst_jsd = 0\n",
    "\n",
    "## Hope\n",
    "for dim in DIM:\n",
    "    for sim in ['katz','aa','cn','ppr']:\n",
    "        X = Hope(g,sim=sim,dim=dim) \n",
    "        saveEmbedding(X,g)\n",
    "        jsd = JS(datadir+'ABCD/abcd_100.dat',datadir+'ABCD/abcd_100.ecg','_embed')\n",
    "        ## keep track of best and worst\n",
    "        if jsd < best_jsd:\n",
    "            os.system('cp _embed _embed_best')\n",
    "            best_jsd = jsd\n",
    "        if jsd > worst_jsd:\n",
    "            os.system('cp _embed _embed_worst')\n",
    "            worst_jsd = jsd\n",
    "        L.append([dim,'hope',sim,jsd])\n",
    "\n",
    "## LE\n",
    "for dim in DIM:\n",
    "    X = LE(g,dim=dim)\n",
    "    saveEmbedding(X,g)\n",
    "    jsd = JS(datadir+'ABCD/abcd_100.dat',datadir+'ABCD/abcd_100.ecg','_embed')\n",
    "    ## keep track of best and worst\n",
    "    if jsd < best_jsd:\n",
    "        os.system('cp _embed _embed_best')\n",
    "        best_jsd = jsd\n",
    "    if jsd > worst_jsd:\n",
    "        os.system('cp _embed _embed_worst')\n",
    "        worst_jsd = jsd\n",
    "    L.append([dim,'le',' ',jsd])\n",
    "    \n",
    "## node2vec is in my path\n",
    "for dim in DIM:\n",
    "    for (p,q) in [(1,0.1),(1,.5),(0.1,1),(.5,1),(1,1)]:\n",
    "        x = 'node2vec -i:'+datadir+'ABCD/abcd_100.dat -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        ## if you get unstable results, you can try with shorter random walks, for example:\n",
    "        ## x = 'node2vec -l:10 -i:'+datadir+'ABCD/abcd_100.dat -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        r = os.system(x)\n",
    "        jsd = JS(datadir+'ABCD/abcd_100.dat',datadir+'ABCD/abcd_100.ecg','_embed')\n",
    "        ## keep track of best and worst\n",
    "        if jsd < best_jsd:\n",
    "            os.system('cp _embed _embed_best')\n",
    "            best_jsd = jsd\n",
    "        if jsd > worst_jsd:\n",
    "            os.system('cp _embed _embed_worst')\n",
    "            worst_jsd = jsd\n",
    "        L.append([dim,'n2v',str(p)+' '+str(q),jsd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = pd.DataFrame(L,columns=['dim','algo','param','jsd'])\n",
    "D = D.sort_values(by='jsd',axis=0)\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cp _embed_best _embed')\n",
    "l = embed2layout()\n",
    "g.vs['ly'] = [l[int(v['name'])-1] for v in g.vs]\n",
    "ig.plot(g, layout=g.vs['ly'], bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cp _embed_worst _embed')\n",
    "l = embed2layout()\n",
    "g.vs['ly'] = [l[int(v['name'])-1] for v in g.vs]\n",
    "ig.plot(g, layout=g.vs['ly'], bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large ABCD graph -- find a good embedding with the framework\n",
    "* we ran the code below and saved the best embedding in datdadir+\"ABCD/abcd_1000_embed_best\" for graph with xi=0.6\n",
    "* this can be re-run by uncommenting the cell below\n",
    "* we'll consider more embeddings in the large classification experiment later"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "## this is slow - we only run a few combinations with Hope\n",
    "## store best one in *_embed_best (48-dim with 'ppr')\n",
    "L = []\n",
    "jsd_best = 1\n",
    "DIM = [8,16,32,48]\n",
    "\n",
    "## Hope\n",
    "for dim in DIM:\n",
    "    for sim in ['katz','ppr','aa','cn']:\n",
    "        X = Hope(G, sim=sim, dim=dim) \n",
    "        saveEmbedding(X,G)\n",
    "        jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed')\n",
    "        L.append([dim,'hope',sim,jsd])\n",
    "        if jsd<jsd_best:\n",
    "            jsd_best=jsd\n",
    "            os.system('cp _embed abcd_1000_embed_best')\n",
    "            print(dim,sim)\n",
    "\n",
    "## store in dataframe and show top one\n",
    "os.system('rm _embed')\n",
    "D = pd.DataFrame(L,columns=['dim','algo','param','jsd'])\n",
    "D = D.sort_values(by='jsd',axis=0)\n",
    "D.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on larger ABCD graph\n",
    "\n",
    "* we use a good saved embedding \n",
    "* we use a random forest model on embedded space\n",
    "* we split the data as train and test\n",
    "* the goal is to recover the communities for each node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used the saved \"best\" embedding from above\n",
    "X = readEmbedding(fn=datadir+\"ABCD/abcd_1000_embed_best\")\n",
    "y = G.vs['comm']\n",
    "## train/test split\n",
    "np.random.seed(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Class predictions on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "## percent correct -- this can vary slightly as we split train/test randomly\n",
    "print('\\naccuracy:',sum(cm.diagonal())/sum(sum(cm)),'\\n')\n",
    "#print(bmatrix(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with random classifier -- assuming we know only the number of classes (12)\n",
    "y_pred = [x+1 for x in np.random.choice(12,size=len(y_test),replace=True)]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "## percent correct\n",
    "print('\\nAccuracy:',sum(cm.diagonal())/sum(sum(cm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with random classifier -- using class proportions in training data\n",
    "ctr = Counter(y_train)\n",
    "x = [ctr[i+1] for i in range(12)]\n",
    "s = np.sum(x)\n",
    "p = [i/s for i in x]\n",
    "y_pred = [x+1 for x in np.random.choice(12,size=len(y_test),replace=True,p=p)]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "## percent correct\n",
    "print('\\nAccuracy:',sum(cm.diagonal())/sum(sum(cm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "* we run graph clustering (Louvain, ECG)\n",
    "* we compare with vector space embedding using same embedding\n",
    "* we use k-means (various k) and DBSCAN\n",
    "* recall there are 12 ground truth communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## again we use 'good' embedding from before\n",
    "X = readEmbedding(fn=datadir+\"ABCD/abcd_1000_embed_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "K = [6,9,12,15,24] ## for k-means (real number of clusters is 12)\n",
    "REP = 30\n",
    "\n",
    "for i in range(REP):\n",
    "    \n",
    "    ## kmeans\n",
    "    for k in K:\n",
    "        cl = KMeans(n_clusters=k).fit(X)\n",
    "        d = {k:v for k,v in enumerate(cl.labels_)}\n",
    "        scr = CHS(X,cl.labels_)\n",
    "        ami = AMI(list(GT.values()),list(d.values()))\n",
    "        L.append(['km'+str(k),scr,ami])\n",
    "\n",
    "    ## ECG\n",
    "    ec = G.community_ecg().membership\n",
    "    scr = G.modularity(ec)\n",
    "    ami = AMI(list(GT.values()),ec)\n",
    "    L.append(['ecg',scr,ami])\n",
    "    \n",
    "    ## Louvain -- permute as this is not done in igraph\n",
    "    p = np.random.permutation(G.vcount()).tolist()\n",
    "    GG = G.permute_vertices(p)\n",
    "    l = GG.community_multilevel().membership\n",
    "    ll = [-1]*len(l)\n",
    "    for i in range(len(l)):\n",
    "        ll[i] = l[p[i]]\n",
    "    scr = G.modularity(ll)\n",
    "    ami = AMI(list(GT.values()),ll)\n",
    "    L.append(['ml',scr,ami])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## results with best score for 3 algorithms\n",
    "D = pd.DataFrame(L,columns=['algo','scr','ami'])\n",
    "\n",
    "x = list(D[[x.startswith('km') for x in D['algo']]].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('K-Means:',x)\n",
    "\n",
    "x = list(D[D['algo']=='ml'].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('Louvain:',x)\n",
    "\n",
    "x = list(D[D['algo']=='ecg'].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('ECG:',x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot AMI results\n",
    "A = []\n",
    "algo = ['km6','km9','km12','km15','km24','ml','ecg']\n",
    "for a in algo:\n",
    "    A.append(D[D['algo']==a]['ami'])\n",
    "\n",
    "B = pd.DataFrame(np.transpose(A), \n",
    "                 columns=['k-means(6)','k-means(9)','k-means(12)','k-means(15)',\n",
    "                          'k-means(24)','Louvain','ECG'])\n",
    "B.boxplot(rot=30,figsize=(7,5))\n",
    "plt.ylabel('Adjusted Mutual Information (AMI)');\n",
    "#plt.savefig('embed_cluster.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## DBSCAN -- we tried a few min_sample and dim, good results with 8 and 16 resp.\n",
    "## try various epsilon and test via calinski_harabasz_score (CHS) or silhouette_score \n",
    "top = 0\n",
    "for dim in [16]:\n",
    "    for ms in [8]:\n",
    "        U = umap.UMAP(n_components=dim).fit_transform(X)\n",
    "        for e in np.arange(.4,.5,.0025):\n",
    "            cl = DBSCAN(eps=e, min_samples=ms ).fit(U)\n",
    "            labels = cl.labels_\n",
    "            s = CHS(U,labels) ## score\n",
    "            if s > top:\n",
    "                top=s\n",
    "                e_top=e\n",
    "                d_top=dim\n",
    "                m_top=ms\n",
    "U = umap.UMAP(n_components=d_top).fit_transform(X)\n",
    "cl = DBSCAN(eps=e_top, min_samples=m_top).fit(U)\n",
    "\n",
    "b = [x>-1 for x in cl.labels_]\n",
    "l = list(GT.values())\n",
    "v = [l[i] for i in range(len(l)) if b[i]]\n",
    "print('AMI without outliers:',AMI(v,cl.labels_[b]))\n",
    "print('AMI with outliers:',AMI(list(GT.values()),cl.labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "\n",
    "* we drop 10% edges and re-compute the embedding\n",
    "* we train a logistic regression model\n",
    "* we apply final model to test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try with noisy graph\n",
    "\n",
    "Recall that xi=0.6, the proportion of noise edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10% edges at random, save new graph as Gp\n",
    "test_size = int(np.round(.1*G.ecount()))\n",
    "np.random.seed(123456)\n",
    "test_eid = np.random.choice(G.ecount(),size=test_size,replace=False)\n",
    "Gp = G.copy()\n",
    "Gp.delete_edges(test_eid)\n",
    "\n",
    "## compute embedding on Gp with parameters that yielded a good embedding for G\n",
    "X = Hope(Gp,sim='ppr', dim=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model with Hadamard binary operator (other choices are 'l1', 'l2 and 'avg')\n",
    "op = 'had'\n",
    "\n",
    "## Build training data, first the edges\n",
    "F = []\n",
    "for e in Gp.es:\n",
    "    F.append(binary_operator(X[e.tuple[0]],X[e.tuple[1]],op=op))\n",
    "size = len(F)\n",
    "f = [1]*size\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [tuple(np.random.choice(Gp.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "e = [(min(x),max(x)) for x in e if Gp.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "non_edges = list(set(e))[:size]\n",
    "for e in non_edges:\n",
    "    F.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "F = np.array(F)\n",
    "f.extend([0]*size)\n",
    "\n",
    "## train model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(F,f)\n",
    "\n",
    "## prepare test set, first with all dropped edges from G \n",
    "X_test = []\n",
    "for i in test_eid:\n",
    "    e = G.es[i]\n",
    "    X_test.append(binary_operator(X[e.tuple[0]],X[e.tuple[1]],op=op))\n",
    "size = len(X_test)\n",
    "y_test = [1]*size\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [tuple(np.random.choice(G.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "e = [(min(x),max(x)) for x in e if G.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "non_edges = list(set(e))[:size]\n",
    "for e in non_edges:\n",
    "    X_test.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "X_test = np.array(X_test)\n",
    "y_test.extend([0]*size)\n",
    "\n",
    "## apply the model to test data\n",
    "print('Accuracy of logistic regression classifier with',op,'on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print('AUC:',roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This is better than random, but not great; \n",
    "## recall that xi=0.6, so the majority of edges are noise to start with;\n",
    "## link prediction is very hard in this case\n",
    "## we try with less noisy graph below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction with less noisy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities - graph with xi=0.2\n",
    "G2 = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_1000_xi2.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_1000_xi2_comms.dat',dtype='uint16',usecols=(1))\n",
    "G2.vs['comm'] = [c[int(x['name'])-1] for x in G2.vs]\n",
    "\n",
    "## pick 10% edges at random, save new graph as Gp\n",
    "test_size = int(np.round(.1*G2.ecount()))\n",
    "np.random.seed(123456) ## for reproducibility\n",
    "test_eid = np.random.choice(G2.ecount(),size=test_size,replace=False)\n",
    "Gp = G2.copy()\n",
    "Gp.delete_edges(test_eid)\n",
    "\n",
    "## compute embedding on Gp with parameters that yielded a good embedding for G2\n",
    "X = Hope(Gp,sim='ppr', dim=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model with Hadamard binary operator (other choices are 'l1', 'l2 and 'avg')\n",
    "op = 'had'\n",
    "\n",
    "## Build training data, first the edges\n",
    "F = []\n",
    "for e in Gp.es:\n",
    "    F.append(binary_operator(X[e.tuple[0]],X[e.tuple[1]],op=op))\n",
    "size = len(F)\n",
    "f = [1]*size\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [tuple(np.random.choice(Gp.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "e = [(min(x),max(x)) for x in e if Gp.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "non_edges = list(set(e))[:size]\n",
    "for e in non_edges:\n",
    "    F.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "F = np.array(F)\n",
    "f.extend([0]*size)\n",
    "\n",
    "## train model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(F,f)\n",
    "\n",
    "## prepare test set, first with all dropped edges from G \n",
    "X_test = []\n",
    "for i in test_eid:\n",
    "    e = G2.es[i]\n",
    "    X_test.append(binary_operator(X[e.tuple[0]],X[e.tuple[1]],op=op))\n",
    "size = len(X_test)\n",
    "y_test = [1]*size\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [tuple(np.random.choice(G2.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "e = [(min(x),max(x)) for x in e if G2.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "non_edges = list(set(e))[:size]\n",
    "for e in non_edges:\n",
    "    X_test.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "X_test = np.array(X_test)\n",
    "y_test.extend([0]*size)\n",
    "\n",
    "## apply the model to test data\n",
    "print('Accuracy of logistic regression classifier with',op,'on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print('AUC:',roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='gray',label='Logistic Regression (AUC = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('embed_link.eps')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger study -- use classification accuracy for picking embedding\n",
    "\n",
    "- we use training-validation-test split\n",
    "- this can be long to run -- a pickle file with the results is included in data directory\n",
    "- to re-run from scratch, uncomment the next cell; results can differ in that case due to non-deterministic algorithms like node2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train/val/test, split the id's in proportion 25/25/50\n",
    "np.random.seed(1)\n",
    "ids = [i for i in range(G.vcount())]\n",
    "id_trainval, id_test = train_test_split(ids, test_size=.5)     ## split test\n",
    "id_train, id_val = train_test_split(id_trainval, test_size=.5) ## split train/val\n",
    "\n",
    "y_all = G.vs['comm']\n",
    "y_train = [y_all[i] for i in id_train]\n",
    "y_trainval = [y_all[i] for i in id_trainval]\n",
    "y_val = [y_all[i] for i in id_val]\n",
    "y_test = [y_all[i] for i in id_test]\n",
    "\n",
    "## loop over several algos, parameters\n",
    "L = []\n",
    "DIM = [2,4,8,16,24,32,48]\n",
    "\n",
    "## LE\n",
    "for dim in DIM:\n",
    "    print(dim)\n",
    "    X = LE(G, dim=dim)\n",
    "    X_train = X[id_train,:]\n",
    "    X_val = X[id_val,:]\n",
    "    saveEmbedding(X,G)\n",
    "    jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed')\n",
    "\n",
    "    # Create the model with 100 trees\n",
    "    model = RandomForestClassifier(n_estimators=100, \n",
    "                                   bootstrap = True,\n",
    "                                   max_features = 'sqrt')\n",
    "    # Fit on training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Actual class predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    scr = accuracy_score(y_val,y_pred)\n",
    "    L.append([dim,'le',0,jsd,scr])    \n",
    "    \n",
    "## HOPE\n",
    "for dim in DIM:\n",
    "    print(dim)\n",
    "    for sim in ['katz','aa','cn','ppr']:    \n",
    "        X = Hope(G,sim=sim,dim=dim) \n",
    "        X_train = X[id_train,:]\n",
    "        X_val = X[id_val,:]\n",
    "        saveEmbedding(X,G)\n",
    "        jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed')\n",
    "\n",
    "        # Create the model with 100 trees\n",
    "        model = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap = True,\n",
    "                                       max_features = 'sqrt')\n",
    "        # Fit on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Actual class predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        scr = accuracy_score(y_val,y_pred)\n",
    "        L.append([dim,'hope',sim,jsd,scr])\n",
    "\n",
    "## node2vec\n",
    "## node2vec is in my path\n",
    "for dim in DIM:\n",
    "    print(dim)\n",
    "    for (p,q) in [(1,0.01),(1,.5),(0.01,1),(.5,1),(1,1)]:\n",
    "        x = 'node2vec -i:'+datadir+'ABCD/abcd_1000.dat -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        r = os.system(x)\n",
    "        X = readEmbedding(N2K=N2K)\n",
    "        jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed')\n",
    "        X_train = X[id_train,:]\n",
    "        X_val = X[id_val,:]\n",
    "        # Create the model with 100 trees\n",
    "        model = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap = True,\n",
    "                                       max_features = 'sqrt')\n",
    "\n",
    "        # Fit on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Actual class predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        scr = accuracy_score(y_val,y_pred)\n",
    "        L.append([dim,'n2v',str(p)+' '+str(q),jsd,scr])\n",
    "\n",
    "## save L and train/val/test ids\n",
    "pickle.dump( (id_train,id_val,id_trainval,id_test,L), open( datadir+\"ABCD/abcd_1000_embeddings.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load L and train/val/test ids\n",
    "with open(datadir+\"ABCD/abcd_1000_embeddings.pkl\",\"rb\") as f:\n",
    "    id_train,id_val,id_trainval,id_test,L = pickle.load(f)\n",
    "y_all = G.vs['comm']\n",
    "y_train = [y_all[i] for i in id_train]\n",
    "y_trainval = [y_all[i] for i in id_trainval]\n",
    "y_val = [y_all[i] for i in id_val]\n",
    "y_test = [y_all[i] for i in id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pd.DataFrame(L,columns=['dim','algo','param','div','acc'])\n",
    "from scipy.stats import kendalltau as tau\n",
    "print(tau(R['div'],R['acc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Divergence on validation set\n",
    "R = R.sort_values(by='div',axis=0,ascending=True)\n",
    "size = R.shape[0]\n",
    "R['rank_div'] = np.arange(1,size+1,1)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Accuracy on validation set\n",
    "R = R.sort_values(by='acc',axis=0,ascending=False)\n",
    "size = R.shape[0]\n",
    "R['rank_acc'] = np.arange(1,size+1,1)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## quite a range of accuracy on the validation set!\n",
    "R.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apply to test set. \n",
    "\n",
    "This takes several minutes to run so a pickle file is provided.\n",
    "\n",
    "Uncomment the cell below to re-run; results can differ in that case due to non-deterministic algorithms like node2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## retrain and score in order of validation set's accuracy\n",
    "top_acc = []\n",
    "for i in range(size):\n",
    "    dim, algo, param, div, acc, rk_a, rk_d = R.iloc[i]\n",
    "    if algo=='n2v':\n",
    "        s = param.split()\n",
    "        p = float(s[0])\n",
    "        q = float(s[1])\n",
    "        x = 'node2vec -i:'+datadir+'ABCD/abcd_1000.dat -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        r = os.system(x)\n",
    "        X = readEmbedding(N2K=N2K)\n",
    "    if algo=='hope':\n",
    "        X = Hope(G,sim=param,dim=dim)\n",
    "    if algo=='le':\n",
    "        X = LE(G, dim=dim)\n",
    "        \n",
    "    X_trainval = X[id_trainval,:]\n",
    "    X_test = X[id_test,:]\n",
    "    # Create the model with 100 trees\n",
    "    model = RandomForestClassifier(n_estimators=100, \n",
    "                                   bootstrap = True,\n",
    "                                   max_features = 'sqrt')\n",
    "    # Fit on training data\n",
    "    model.fit(X_trainval, y_trainval)\n",
    "\n",
    "    # Actual class predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    scr = accuracy_score(y_test,y_pred)\n",
    "    top_acc.append(scr)\n",
    "\n",
    "pickle.dump( top_acc, open( datadir+\"ABCD/abcd_1000_embeddings_test_BIS.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test results\n",
    "with open(datadir+\"ABCD/abcd_1000_embeddings_test.pkl\",\"rb\") as f:\n",
    "    top_acc = pickle.load(f)\n",
    "R['test'] = top_acc\n",
    "print('mean accuracy over all models on test set:',np.mean(R['test']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R.sort_values(by='test',axis=0,ascending=False)\n",
    "R['rank_test'] = np.arange(1,size+1,1)\n",
    "R.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top results on test set w.r.t. divergence on validation set\n",
    "R = R.sort_values(by='div',axis=0,ascending=True)\n",
    "top_div = R['test'][:10]\n",
    "\n",
    "## top results on test set w.r.t. accuracy on validation set\n",
    "R = R.sort_values(by='acc',axis=0,ascending=False)\n",
    "top_acc = R['test'][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pd with mu\n",
    "B = pd.DataFrame(np.transpose(np.array([top_acc,top_div])), \n",
    "                 columns=['Top-10 validation set accuracy','Top-10 divergence score'])\n",
    "B.boxplot(rot=0,figsize=(7,5), widths=.33)\n",
    "plt.ylabel('Test set accuracy',fontsize=14);\n",
    "#plt.savefig('embed_classify.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R['rank_acc'],R['test'],'.',color='black')\n",
    "plt.xlabel('Rank',fontsize=14)\n",
    "plt.ylabel('Test set accuracy',fontsize=14);\n",
    "#plt.savefig('rank_accuracy.eps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R['rank_div'],R['test'],'.',color='black')\n",
    "plt.xlabel('Rank',fontsize=14)\n",
    "plt.ylabel('Test set accuracy',fontsize=14);\n",
    "#plt.savefig('rank_divergence.eps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## negative correlations\n",
    "print(np.corrcoef(R['rank_acc'],R['test'])[0,1],\n",
    "      np.corrcoef(R['rank_div'],R['test'])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random classification -- AMI\n",
    "ctr = Counter(y_trainval)\n",
    "x = [ctr[i+1] for i in range(12)]\n",
    "s = np.sum(x)\n",
    "p = [i/s for i in x]\n",
    "y_pred = [x+1 for x in np.random.choice(12,size=len(y_test),replace=True,p=p)]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nRandom classifier accuracy on test set:',sum(cm.diagonal())/sum(sum(cm)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReFex: illustrate roles on Zachary graph\n",
    "\n",
    "We use the 'graphrole' package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "feature_extractor = RecursiveFeatureExtractor(z, max_generations=4)\n",
    "features = feature_extractor.extract_features()\n",
    "print(f'\\nFeatures extracted from {feature_extractor.generation_count} recursive generations:')\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign node roles in a dictionary\n",
    "role_extractor = RoleExtractor(n_roles=3)\n",
    "role_extractor.extract_role_factors(features)\n",
    "node_roles = role_extractor.roles\n",
    "role_extractor.role_percentage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "unique_roles = sorted(set(node_roles.values()))\n",
    "# uncomment for color plot\n",
    "# cls = ['red','blue','green']\n",
    "# map roles to colors\n",
    "role_colors = {role: cls[i] for i, role in enumerate(unique_roles)}\n",
    "\n",
    "# store colors for all nodes in G\n",
    "z.vs()['color'] = [role_colors[node_roles[node]] for node in range(z.vcount())]\n",
    "\n",
    "## Plot with node labels\n",
    "z.vs()['size'] = 10\n",
    "#z.vs()['label'] = [v.index for v in z.vs()]\n",
    "z.vs()['label_size'] = 0\n",
    "#ig.plot(z, 'refex.eps', bbox=(0,0,300,300)) \n",
    "ig.plot(z, bbox=(0,0,300,300)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset -- American College Football Graph\n",
    "\n",
    "[REF]: \"Community structure in social and biological networks\", M. Girvan and M. E. J. Newman\n",
    "PNAS June 11, 2002 99 (12) 7821-7826; https://doi.org/10.1073/pnas.122653799\n",
    "\n",
    "\n",
    "Teams are part of 12 conferences (the 'communities'):\n",
    "*   0 = Atlantic Coast\n",
    "*   1 = Big East\n",
    "*   2 = Big Ten\n",
    "*   3 = Big Twelve\n",
    "*   4 = Conference USA\n",
    "*   5 = Independents\n",
    "*   6 = Mid-American\n",
    "*   7 = Mountain West\n",
    "*   8 = Pacific Ten\n",
    "*   9 = Southeastern\n",
    "*  10 = Sun Belt\n",
    "*  11 = Western Athletic\n",
    "\n",
    "14 teams out of 115 appear as anomalies as can be seen in Figure 5 of [REF], namely:\n",
    "- 5 teams in #5 conference (Independent) play teams in other conferences (green triangles)\n",
    "- 7 teams in #10 conference (Sun Belt) are broken in 2 clumps (pink triangles) \n",
    "- 2 teams from #11 conference play mainly with #10 conference (red triangles)\n",
    "\n",
    "Here, we try to recover those anomalous teams by running several embeddings (we use node2vec):\n",
    "\n",
    "- for each embedding:\n",
    " - compute divergence using our framework\n",
    " - also compute entropy of b-vector for each node (probability distribution of edges w.r.t. every community in the geometric Chung-Lu model)\n",
    "- plot entropy vs divergence\n",
    "- for some good/bad embedding, boxplot entropy of anomalous vs other nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "g = ig.Graph.Read_Ncol(datadir+'Football/football.edgelist',directed=False)\n",
    "c = np.loadtxt(datadir+'Football/football.community',dtype='uint16',usecols=(0))\n",
    "g.vs['community'] = [c[int(x['name'])] for x in g.vs]\n",
    "\n",
    "## Read and plot the College Football Graph\n",
    "g.vs['shape'] = 'circle'\n",
    "g.vs['anomaly'] = 0\n",
    "pal = ig.RainbowPalette(n=max(g.vs['community'])+1) \n",
    "g.vs['color'] = [pal.get(int(i)) for i in g.vs['community']]\n",
    "for v in g.vs:\n",
    "    if v['community'] in [5,10] or v['name'] in ['28','58']:\n",
    "        v['shape']='triangle'\n",
    "        v['anomaly']=1\n",
    "ly = g.layout_fruchterman_reingold()\n",
    "ig.plot(g, layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n",
    "#ig.plot(g, target=\"anomaly_0.eps\", layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greyscale\n",
    "pal = ig.GradientPalette(\"white\",\"black\",max(g.vs['community'])+1)\n",
    "g.vs['color'] = [pal.get(int(i)) for i in g.vs['community']]\n",
    "ig.plot(g, layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n",
    "#ig.plot(g, target=\"anomaly_1.eps\", layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_jsd = 1\n",
    "worst_jsd = 0\n",
    "## node2vec with varying parameters (60 embeddings)\n",
    "L = []\n",
    "for dim in np.arange(2,25,2):\n",
    "    for (p,q) in [(1,0.5),(0.5,1),(1,0.01),(0.01,1),(1,1)]:\n",
    "        x = 'node2vec -i:'+datadir+'Football/football.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        ## if you get unstable results, you can try with shorter random walks, for example:\n",
    "        ## x = 'node2vec -l:15 -i:'+datadir+'Football/football.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "        r = os.system(x)\n",
    "        jsd = JS(datadir+'Football/football.edgelist',datadir+'Football/football.ecg','_embed',entropy=True)\n",
    "        ## keep track of best and worst\n",
    "        if jsd < best_jsd:\n",
    "            os.system('cp _entropy _entropy_best')\n",
    "            best_jsd = jsd\n",
    "        if jsd > worst_jsd:\n",
    "            os.system('cp _entropy _entropy_worst')\n",
    "            worst_jsd = jsd\n",
    "\n",
    "        ent = list(pd.read_csv('_entropy',header=None)[1])\n",
    "        g.vs['ent'] = ent\n",
    "        roc = roc_auc_score(g.vs['anomaly'], ent)\n",
    "        L.append([dim,'n2v',str(p)+' '+str(q),jsd,roc])\n",
    "D = pd.DataFrame(L,columns=['dim','algo','param','jsd','auc'])\n",
    "D = D.sort_values(by='jsd',axis=0)\n",
    "D.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auc vs divergence (jsd)\n",
    "plt.plot(D['jsd'],D['auc'],'o',color='black')\n",
    "plt.xlabel('JS Divergence',fontsize=14)\n",
    "plt.ylabel('AUC',fontsize=14);\n",
    "#plt.savefig('anomaly_2.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entropy scores - some good embedding\n",
    "g.vs['ent'] = list(pd.read_csv('_entropy_best',header=None)[1])\n",
    "X = [v['ent'] for v in g.vs if v['anomaly']==0]\n",
    "Y = [v['ent'] for v in g.vs if v['anomaly']==1]\n",
    "plt.boxplot([X,Y],labels=['Regular','Anomalous'],sym='.',whis=(0,100), widths=.5)\n",
    "plt.title(\"Low divergence embedding\",fontsize=14)\n",
    "plt.ylabel('Entropy',fontsize=14);\n",
    "#plt.savefig('anomaly_3.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entropy scores - some not so good embedding\n",
    "g.vs['ent'] = list(pd.read_csv('_entropy_worst',header=None)[1])\n",
    "X = [v['ent'] for v in g.vs if v['anomaly']==0]\n",
    "Y = [v['ent'] for v in g.vs if v['anomaly']==1]\n",
    "plt.boxplot([X,Y],labels=['Regular','Anomalous'],sym='.',whis=(0,100), widths=.5)\n",
    "plt.title(\"High divergence embedding\",fontsize=14)\n",
    "plt.ylabel('Entropy',fontsize=14);\n",
    "#plt.savefig('anomaly_4.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC using average rank with several top embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "## try with top-k embeddings together - hopefully this consistently yields high AUC\n",
    "k = 7\n",
    "g.vs['rank'] = 0\n",
    "for i in range(k):\n",
    "    dim = D.iloc[i]['dim']\n",
    "    p = float(D.iloc[i]['param'].split()[0])\n",
    "    q = float(D.iloc[i]['param'].split()[1])\n",
    "    x = 'node2vec -i:'+datadir+'Football/football.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "    ## if you get unstable results, you can try with shorter random walks, for example:\n",
    "    ## x = 'node2vec -l:15 -i:'+datadir+'Football/football.edgelist -o:_embed -d:'+str(dim)+' -p:'+str(p)+' -q:'+str(q)\n",
    "    r = os.system(x)\n",
    "    jsd = JS(datadir+'Football/football.edgelist',datadir+'Football/football.ecg','_embed',entropy=True)\n",
    "    g.vs['ent'] = list(pd.read_csv('_entropy',header=None)[1])\n",
    "    rk = rankdata(g.vs['ent'])\n",
    "    for i in range(len(rk)):\n",
    "        g.vs[i]['rank'] += rk[i]\n",
    "print('AUC:',roc_auc_score(g.vs['anomaly'], g.vs['rank']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
