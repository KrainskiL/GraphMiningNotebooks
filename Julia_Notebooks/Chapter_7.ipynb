{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required extra package:\n",
    "\n",
    "For hypergraphs:\n",
    "```\n",
    "using PyCall\n",
    "run(`$(PyCall.python) -m pip install hypernetx`)\n",
    "```\n",
    "\n",
    "Functionality of HypernetX package is described in Python notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "using PyPlot\n",
    "using LightGraphs\n",
    "using GraphPlot\n",
    "using DataFrames\n",
    "using Random\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnx = pyimport(\"hypernetx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = pyimport(\"igraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_igraph = pyimport(\"partition_igraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data directory\n",
    "datadir=\"../Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ig2lg(ig_g)\n",
    "    lg_g = SimpleGraph(ig_g.vcount())\n",
    "    for e in ig_g.es()\n",
    "        add_edge!(lg_g, e.source + 1, e.target + 1)\n",
    "    end\n",
    "    return lg_g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for HNX nypergraphs as described above:\n",
    "## We keep this code in Python as we are updating Python objects using it\n",
    "\n",
    "py\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import igraph as ig\n",
    "\n",
    "def factorial(n): \n",
    "    if n < 2: return 1\n",
    "    return reduce(lambda x, y: x*y, range(2, int(n)+1))\n",
    "\n",
    "## Precompute some values on HNX hypergraph for computing qH faster\n",
    "def HNX_precompute(HG):\n",
    "    ## 1. compute node strenghts (weighted degrees)\n",
    "    for v in HG.nodes:\n",
    "        HG.nodes[v].strength = 0\n",
    "    for e in HG.edges:\n",
    "        try:\n",
    "            w = HG.edges[e].weight\n",
    "        except:\n",
    "            w = 1\n",
    "            ## add unit weight if none to simplify other functions\n",
    "            HG.edges[e].weight = 1 \n",
    "        for v in list(HG.edges[e]):\n",
    "            HG.nodes[v].strength += w\n",
    "    ## 2. compute d-weights        \n",
    "    ctr = Counter([len(HG.edges[e]) for e in HG.edges])\n",
    "    for k in ctr.keys():\n",
    "        ctr[k]=0\n",
    "    for e in HG.edges:\n",
    "        ctr[len(HG.edges[e])] += HG.edges[e].weight\n",
    "    HG.d_weights = ctr\n",
    "    HG.total_weight = sum(ctr.values())\n",
    "    ## 3. compute binomial coeffcients (modularity speed-up)\n",
    "    bin_coef = {}\n",
    "    for n in HG.d_weights.keys():\n",
    "        for k in np.arange(n//2+1,n+1):\n",
    "            bin_coef[(n,k)] = factorial(n)/(factorial(k)*factorial(n-k))\n",
    "    HG.bin_coef = bin_coef\n",
    "\n",
    "#########################################\n",
    "\n",
    "## default: linear w.r.t. c\n",
    "def linear(d,c):\n",
    "    return c/d if c>d/2 else 0\n",
    "\n",
    "## majority\n",
    "def majority(d,c):\n",
    "    return 1 if c>d/2 else 0\n",
    "\n",
    "## strict\n",
    "def strict(d,c):\n",
    "    return 1 if c==d else 0\n",
    "\n",
    "#########################################\n",
    "\n",
    "## compute vol(A_i)/vol(V) for each part A_i in A (list of sets)\n",
    "def compute_partition_probas(HG, A):\n",
    "    p = []\n",
    "    for part in A:\n",
    "        vol = 0\n",
    "        for v in part:\n",
    "            vol += HG.nodes[v].strength\n",
    "        p.append(vol)\n",
    "    s = sum(p)\n",
    "    return [i/s for i in p]\n",
    "\n",
    "## degree tax \n",
    "def DegreeTax(HG, Pr, wdc):\n",
    "    DT = 0\n",
    "    for d in HG.d_weights.keys():\n",
    "        tax = 0\n",
    "        for c in np.arange(d//2+1,d+1):\n",
    "            for p in Pr:\n",
    "                tax += p**c * (1-p)**(d-c) * HG.bin_coef[(d,c)] * wdc(d,c)\n",
    "        tax *= HG.d_weights[d]\n",
    "        DT += tax\n",
    "    DT /= HG.total_weight\n",
    "    return DT\n",
    "\n",
    "## edge contribution, A is list of sets\n",
    "def EdgeContribution(HG, A, wdc):\n",
    "    EC = 0\n",
    "    for e in HG.edges:\n",
    "        d = HG.size(e)\n",
    "        for part in A:\n",
    "            if HG.size(e,part) > d/2:\n",
    "                EC += wdc(d,HG.size(e,part)) * HG.edges[e].weight\n",
    "    EC /= HG.total_weight\n",
    "    return EC\n",
    "\n",
    "## HG: HNX hypergraph\n",
    "## A: partition (list of sets)\n",
    "## wcd: weight function (ex: strict, majority, linear)\n",
    "def HNX_modularity(HG, A, wdc=linear):\n",
    "    Pr = compute_partition_probas(HG, A)\n",
    "    return EdgeContribution(HG, A, wdc) - DegreeTax(HG, Pr, wdc)\n",
    "\n",
    "#########################################\n",
    "\n",
    "## 2-section igraph from HG\n",
    "def HNX_2section(HG):\n",
    "    s = []\n",
    "    for e in HG.edges:\n",
    "        E = HG.edges[e]\n",
    "         ## random-walk 2-section (preserve nodes' weighted degrees)\n",
    "        try:\n",
    "            w = HG.edges[e].weight/(len(E)-1)\n",
    "        except:\n",
    "            w = 1/(len(E)-1)\n",
    "        s.extend([(k[0],k[1],w) for k in itertools.combinations(E,2)])\n",
    "    G = ig.Graph.TupleList(s,weights=True).simplify(combine_edges='sum')\n",
    "    return G\n",
    "\n",
    "#########################################\n",
    "\n",
    "## we use 2 representations for partitions (0-based part ids):\n",
    "## (1) dictionary or (2) list of sets\n",
    "\n",
    "def dict2part(D):\n",
    "    P = []\n",
    "    k = list(D.keys())\n",
    "    v = list(D.values())\n",
    "    for x in range(max(D.values())+1):\n",
    "        P.append(set([k[i] for i in range(len(k)) if v[i]==x]))\n",
    "    return P\n",
    "\n",
    "def part2dict(A):\n",
    "    x = []\n",
    "    for i in range(len(A)):\n",
    "        x.extend([(a,i) for a in A[i]])\n",
    "    return {k:v for k,v in x}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy hypergraph example with HNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "## using 'enumerate', edges will have integer IDs\n",
    "E = [Set([\"A\",\"B\"]),Set([\"A\",\"C\"]),Set([\"A\",\"B\",\"C\"]),Set([\"A\",\"D\",\"E\",\"F\"]),Set([\"D\",\"F\"]),Set([\"E\",\"F\"])]\n",
    "HG = hnx.Hypergraph(Dict(enumerate(E)))\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "hnx.draw(HG, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "HD = HG.dual()\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "hnx.draw(HD, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute node strength (add unit weight if none), d-degrees, binomial coefficients\n",
    "py\"HNX_precompute\"(HG)\n",
    "## show the edges (unit weights were added by default)\n",
    "HG.edges.elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the nodes (here strength = degree since all weights are 1)\n",
    "HG.nodes.elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d-weights distribution\n",
    "HG.d_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute modularity qH for the following partitions:\n",
    "A1 = [Set([\"A\",\"B\",\"C\"]),Set([\"D\",\"E\",\"F\"])]\n",
    "A2 = [Set([\"B\",\"C\"]),Set([\"A\",\"D\",\"E\",\"F\"])]\n",
    "A3 = [Set([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"])]\n",
    "A4 = [Set([\"A\"]),Set([\"B\"]),Set([\"C\"]),Set([\"D\"]),Set([\"E\"]),Set([\"F\"])]\n",
    "\n",
    "println(\"linear: \", [py\"HNX_modularity\"(HG,A1), py\"HNX_modularity\"(HG,A2),\n",
    "                     py\"HNX_modularity\"(HG,A3), py\"HNX_modularity\"(HG,A4)])\n",
    "println(\"strict: \", [py\"HNX_modularity\"(HG,A1, py\"strict\"), py\"HNX_modularity\"(HG,A2,py\"strict\"),\n",
    "                     py\"HNX_modularity\"(HG,A3,py\"strict\"), py\"HNX_modularity\"(HG,A4,py\"strict\")])\n",
    "println(\"majority: \", [py\"HNX_modularity\"(HG,A1,py\"majority\"), py\"HNX_modularity\"(HG,A2,py\"majority\"),\n",
    "                       py\"HNX_modularity\"(HG,A3,py\"majority\"), py\"HNX_modularity\"(HG,A4,py\"majority\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = py\"HNX_2section\"(HG)\n",
    "gplot(ig2lg(G),\n",
    "      NODESIZE=0.05, nodefillc=\"red\",\n",
    "      nodelabel=G.vs.get_attribute_values(\"name\"),\n",
    "      nodelabelc=\"black\",\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = G.community_ecg().membership\n",
    "[Set(G.vs.get_attribute_values(\"name\")[m .== v]) for v in unique(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section clustering with ECG\n",
    "G.vs['community'] = G.community_ecg().membership\n",
    "dict2part({v['name']:v['community'] for v in G.vs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "REF: https://github.com/jeffreylancaster/game-of-thrones\n",
    "\n",
    "We built an hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving those characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py\"\"\"\n",
    "import pickle\n",
    "\n",
    "datadir='../Datasets/'\n",
    "\n",
    "with open(datadir+\"GoT/GoT.pkl\", \"rb\") as f:\n",
    "    Edges, Names, Weights = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build weighted hypergraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py\"\"\"\n",
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "HG = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "## add edge weights\n",
    "for e in HG.edges:\n",
    "    HG.edges[e].weight = Weights[e]\n",
    "## add full names\n",
    "for v in HG.nodes:\n",
    "    HG.nodes[v].name = Names[v]\n",
    "## pre-compute required quantities for modularity and clustering\n",
    "HNX_precompute(HG)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(py\"HG\".number_of_nodes(), \" nodes and \", py\"HG\".number_of_edges(), \" edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on GoT hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "hist(py\"[HG.edges[e].size() for e in HG.edges]\", bins=25, color=\"grey\")\n",
    "xlabel(\"Edge size\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters)\n",
    "hist(py\"[HG.edges[e].weight for e in HG.edges]\", bins=25, color=\"grey\")\n",
    "xlabel(\"Edge weight\",fontsize=14);\n",
    "print(\"max = \",maximum(py\"[HG.edges[e].weight for e in HG.edges]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## node degrees\n",
    "hist(hnx.degree_dist(py\"HG\"),bins=20, color=\"grey\")\n",
    "xlabel(\"Node degree\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node strength (total appearance)\n",
    "hist(py\"[HG.nodes[n].strength for n in HG.nodes]\", bins=20, color=\"grey\")\n",
    "xlabel(\"Node strength\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build dataframe with node characteristics\n",
    "D = DataFrame(name = py\"[HG.nodes[v].name for v in HG.nodes()]\",\n",
    "              degree = py\"[HG.degree(v) for v in HG.nodes()]\",\n",
    "              strength = py\"[HG.nodes[v].strength for v in HG.nodes()]\")\n",
    "sort(D, :strength, rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(D, :degree, rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(D.degree,D.strength, \".\")\n",
    "xlabel(\"degree\", fontsize=14)\n",
    "ylabel(\"strength\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "py\"\"\"\n",
    "G = HNX_2section(HG)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check -- node ordering \n",
    "\n",
    "py\"\"\"\n",
    "## ordering of nodes in HG\n",
    "ord_HG = list(HG.nodes.elements.keys())\n",
    "\n",
    "## ordering of nodes in G\n",
    "ord_G = [v['name'] for v in G.vs]\n",
    "\"\"\"\n",
    "py\"ord_HG\" == py\"ord_G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = py\"G\".betweenness(directed=false,weights=\"weight\")\n",
    "n = py\"G\".vcount()\n",
    "D.betweenness = [2*x/((n-1)*(n-2)) for x in b]\n",
    "D.pagerank = py\"G\".pagerank(directed=false, weights=\"weight\")\n",
    "sort(D, :strength, rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(D, :betweenness, rev=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nodes: \",py\"G\".vcount(),\" edges: \",py\"G\".ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the 2-section graph\n",
    "py\"\"\"\n",
    "G.vs['size'] = 10\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [HG.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 5\n",
    "\"\"\"\n",
    "Random.seed!(1234)\n",
    "gplot(ig2lg(py\"G\"),\n",
    "      NODESIZE=0.04, nodefillc=\"gray\",\n",
    "      nodelabel=py\"G\".vs.get_attribute_values(\"name\"),\n",
    "      nodelabelc=\"black\",\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a small clique: Braavosi theater troup\n",
    "print(py\"[HG.nodes[str(x)].name for x in np.arange(166,173)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modularity (qH) on several random partition with K parts for a range of K's\n",
    "## This should be close to 0 and can be negative.\n",
    "py\"\"\"\n",
    "h = []\n",
    "for K in np.arange(2,21):\n",
    "    for rep in range(10):\n",
    "        V = list(HG.nodes)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## compute qH\n",
    "        h.append(HNX_modularity(HG, RandPart))\n",
    "\"\"\"\n",
    "print(\"range for qH: \",minimum(py\"h\"),\" to \",maximum(py\"h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the 2-section graph (with Louvain) and compute qH\n",
    "## We now see qH >> 0\n",
    "py\"\"\"\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "ML = dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "\"\"\"\n",
    "## Compute qH\n",
    "print(py\"HNX_modularity(HG, ML)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.cluster = py\"G.vs['louvain']\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = Dict(0 => \"yellow\", 1=>\"red\", 2=>\"green\", 3=>\"blue\", 4=>\"violet\")\n",
    "\n",
    "Random.seed!(1234)\n",
    "gplot(ig2lg(py\"G\"),\n",
    "      NODESIZE=0.04, nodefillc=[colors[x] for x in D.cluster],\n",
    "      nodelabel=py\"G\".vs.get_attribute_values(\"name\"),\n",
    "      nodelabelc=\"black\",\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = filter(:name => ==(\"Daenerys Targaryen\"), D).cluster[1]\n",
    "sort(filter(:cluster => ==(dt), D), :strength, rev=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs example \n",
    "\n",
    "Using HNX draw function to get patterns from Figure 7.1 in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H1 pattern\n",
    "E = [Set([\"A\",\"B\"]),Set([\"A\",\"C\"]),Set([\"A\",\"D\"]),Set([\"B\",\"D\"]),Set([\"C\",\"D\"])]\n",
    "HG = hnx.Hypergraph(Dict(enumerate(E)))\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "hnx.draw(HG, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H2 pattern\n",
    "E = [Set([\"A\",\"B\", \"C\"]),Set([\"A\",\"D\"]),Set([\"C\",\"D\"])]\n",
    "HG = hnx.Hypergraph(Dict(enumerate(E)))\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "hnx.draw(HG, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H3 pattern\n",
    "E = [Set([\"A\",\"B\", \"C\"]),Set([\"B\", \"C\",\"D\"])]\n",
    "HG = hnx.Hypergraph(Dict(enumerate(E)))\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "hnx.draw(HG, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Counting those patterns -- Table 7.2: see Python codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with simple community random hypergraphs\n",
    "\n",
    "note: qH-based heuristics are still very experimental; we only provide this for illustration\n",
    "\n",
    "* 16 hypergraphs each with 1000 nodes, 1400 edges of size 2 to 8 (200 each)\n",
    "* 10 communities with 0%, 5%, 10% or 15% pure noise edges (mu)\n",
    "* community edge homogeneity (tau) from 0.5 to 1\n",
    "* 3 algorithms:\n",
    " * qG-based Louvain on 2-section\n",
    " * qH-based heuristic clustering algorithm on hypergraph\n",
    " * qH+: same but using true homogeneity (tau)\n",
    "* Experiment results are stored in files taus_xx.pkl with xx in {00, 05, 10, 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results (here mu = .05)\n",
    "py\"\"\"\n",
    "with open( datadir+\"Hypergraph/taus_05.pkl\", \"rb\" ) as f:\n",
    "    results = pickle.load(f)\n",
    "\"\"\"\n",
    "\n",
    "R = combine(groupby(DataFrame(py\"results\", [\"tau\",\"Graph\",\"Hypergraph\",\"Hypergraph+\"]), :tau),\n",
    "            [\"Graph\",\"Hypergraph\",\"Hypergraph+\"] .=> mean, renamecols=false)\n",
    "plot(R.tau,R.Graph,\"o-\",label=\"qG-based\",color=\"red\")\n",
    "plot(R.tau,R.Hypergraph,\"o-\",label=\"qH-based\",color=\"green\")\n",
    "plot(R.tau,R.\"Hypergraph+\",\"o-\",label=\"qH-based (tuned)\",color=\"blue\")\n",
    "xlabel(\"homogeneity tau\",fontsize=14)\n",
    "ylabel(\"AMI\",fontsize=14)\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community hypergraphs\n",
    "\n",
    "We have hyperedge list and communities for 3 random hypergraph with communities, namely:\n",
    "\n",
    "* edges65, comm65: hypergraphs with $\\tau_e = \\lceil(d*0.65)\\rceil$ for all community edges of side $d$\n",
    "* edges85, comm85: hypergraphs with $\\tau_e = \\lceil(d*0.85)\\rceil$ for all community edges of side $d$\n",
    "* edges65_unif, comm65_unif: hypergraphs with $\\tau_e$ chosen uniformly from $\\{\\lceil(d*0.65)\\rceil,...,d\\}$ for all community edges of side $d$\n",
    "\n",
    "All have 1000 nodes, 1400 edges of size 2 to 8 (200 each) 10 communities and noise parameter $\\mu=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load hypergraphs\n",
    "py\"\"\"\n",
    "with open(datadir+\"Hypergraph/hypergraphs.pkl\",\"rb\") as f:\n",
    "    (edges65, comm65, edges85, comm85, edges65_unif, comm65_unif) = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimating tau\n",
    "\n",
    "## pick one of the three hypergraphs\n",
    "py\"\"\"\n",
    "comm = comm65\n",
    "L = edges65\n",
    "\n",
    "## true communities\n",
    "HG = hnx.Hypergraph(dict(enumerate(L)))\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm])/len(e))\n",
    "y = []\n",
    "tv = np.arange(0.501,1,0.025)\n",
    "for t in tv:\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "\"\"\"\n",
    "plot(py\"tv\", py\"y\",\".-\",color=\"red\",label=\"true communities\")\n",
    "\n",
    "## Louvain\n",
    "py\"\"\"\n",
    "G = HNX_2section(HG)\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "ML = dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in ML])/len(e))\n",
    "y = []\n",
    "for t in tv:\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "\"\"\"\n",
    "plot(py\"tv\", py\"y\", \".-\",color=\"black\",label=\"Louvain\")\n",
    "\n",
    "PyPlot.grid()\n",
    "ylabel(\"Pr(homogeneity > tau\",fontsize=14)\n",
    "xlabel(\"tau\",fontsize=14)\n",
    "legend()\n",
    "ylim(0,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity -- single value for 'tau'\n",
    "py\"\"\"\n",
    "x = []\n",
    "for e in edges65:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm65])/len(e))\n",
    "\"\"\"\n",
    "hist(py\"x\",bins=\"rice\",color=\"grey\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity -- range for 'tau' \n",
    "## we see many more pure community edges\n",
    "py\"\"\"\n",
    "x = []\n",
    "for e in edges65_unif:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm65_unif])/len(e))\n",
    "\"\"\"\n",
    "hist(py\"x\", bins=\"rice\",color=\"grey\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
